{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *Goal*: Use social media posts to explore the appplication of text and natural language processing to see what might be learned from online interactions.\n",
    "\n",
    "Specifically, we will retrieve, annotate, process, and interpret Twitter data on health-related issues such as depression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "References:\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n",
    "* The [Tweepy Python API for Twitter](http://www.tweepy.org/)\n",
    "\n",
    "Required Software\n",
    "* [Python 3](https://www.python.org)\n",
    "* [NumPy](http://www.numpy.org) - for preparing data for plotting\n",
    "* [Matplotlib](https://matplotlib.org) - plots and garphs\n",
    "* [jsonpickle](https://jsonpickle.github.io) for storing tweets. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import time\n",
    "import operator\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This module continues the Social Media Data Science module started in [Part 1](SocialMedia - Part 1.ipynb) covering the annotation of tweets. These lessons will continue in [Part 3](SocialMedia - Part 1.ipynb) as we move on to the use of Natural Language processing to analyze the tweets. \n",
    "  \n",
    "Our case study will apply these topics to Twitter discussions of smoking and vaping. Although details of the tools used to access data and the format and content of the data may differ for various services, the strategies and procedures used to analyze the data will generalize to other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Before we dig in, we must grab a bit of code from [Part 1](SocialMedia - Part 1.ipynb):\n",
    "\n",
    "1. The Tweets class used to store the tweets.\n",
    "2. Our twitter API Keys - be sure to copy the keys that you generated when you completed [Part 1](SocialMedia - Part 1.ipynb).\n",
    "3. Configuration of our Twitter connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.contents={}\n",
    "        self.contents['time']=datetime.now()\n",
    "        self.contents['tweets']={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        self.contents['searchTerm']=term\n",
    "        self.contents['time']=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet):\n",
    "        id = tweet['id_str']\n",
    "        tweets=self.contents['tweets']\n",
    "        if id not in tweets.keys():\n",
    "            tweets[id]={}\n",
    "            tweets[id]['tweet']=tweet\n",
    "            tweets[id]['count']=0\n",
    "        # if a count is not provided in the call, increment the count \n",
    "        if count == 0:\n",
    "            tweets[id]['count'] = tweets[id]['count'] +1\n",
    "        else:\n",
    "            tweets[id]['count'] = count\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        id = str(id)\n",
    "        tweets=self.contents['tweets']\n",
    "        if id in tweets:\n",
    "            return tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.contents['tweets'][id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.contents['tweets'])\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        tweets=self.contents['tweets']\n",
    "        for t,entry in tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.contents['tweets'].keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.contents)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        self.contents={}\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.contents=incontents\n",
    "        \n",
    "    def getSearchTerm(self):\n",
    "        return self.contents['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self):\n",
    "        return self.contents['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*REDACT FOLLOWING DETAILS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key='D2L4YZ2YrO1PMix7uKUK63b8H'\n",
    "consumer_secret='losRw9T8zb6VT3TEJ9JHmmhAmn1GXKVj30dkiMv9vjhXuiWek9'\n",
    "access_token='15283934-iggs1hiZAPI2o5sfHWMfjumTF7SvytHPjpPRGf3I6'\n",
    "access_secret='bOvqssxS97PGPwXHQZxk83KtAcDyLhRLgdQaokCdVvwFi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Annotating Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a corpus of tweets, what do we want to do with them? Turning a relatively vague notion into a well-defined research question is often a significant challenge, as examination of the data often reveals both shortcomings and unforeseen opportunities.\n",
    "\n",
    "In our case, we are interested in looking at tweets about depression, but we're not quite sure exactly *what* we are looking for. We have a vague notion that we might learn something interesting, but understanding exactly what that is, and what sort of analyses we might need, will require a bit more work.\n",
    "\n",
    "In situations such as this, we might look at some of the data to form some preliminary impressions of the content. Specifically, we can look at indidividual tweets, assigning them to one or more categories - known as *codes* - based on their content.  We can add categories as needed to capture important ideas that we might want to refer back to. This practice - known as *open coding* allows us to begin to make sense of unfamiliar data sets. \n",
    "\n",
    "This sounds much more complicated than it is. For now, let's read some tweets in from a file collected using the procedures discused in [Part 1](SocialMedia - Part 1.ipynb). We'll then use those tweets to get to work. The tweets are stored in a file called `tweets-smoking.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets =Tweets()\n",
    "tweets.readTweets(\"tweets.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the count, search term, and date of collection to understand the origin of these tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "smoking\n",
      "2018-03-07 16:59:56.921697\n"
     ]
    }
   ],
   "source": [
    "print(tweets.countTweets())\n",
    "print(tweets.getSearchTerm())\n",
    "print(str(tweets.getSearchTime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by taking a look at a subset of the first 20 tweets\n",
    "\n",
    "To get this list, we'll sort the ids of the tweets and take the first 10 in the list, as ordered by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids=list(tweets.getIds())\n",
    "ids.sort()\n",
    "working=[]\n",
    "for i in range(20):\n",
    "    id = ids[i]\n",
    "    working.append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*working* now has 20 tweets ids. Let's start with the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @NBCNews: Police: Teen tried to detonate bomb at Utah school; backpack with device was found smoking; teen had been researching about IS…'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[0]\n",
    "t = tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tweet has several interesting charcteristics.\n",
    "1. it appears to be a retweet.\n",
    "2. It contains a user mention \n",
    "\n",
    "We can model all of these points through relevant annotation. Specifically, we will a new array of codes to each tweet object. This array will contain a list of categorical annotations associated with the tweet.  We add routines to add a single code to a tweet (by ID), to add multiple codes, and to retrieve the list of codes associated with a tweet.\n",
    "\n",
    "\n",
    "See modifications to the  Tweets object in this new definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.contents={}\n",
    "        self.contents['time']=datetime.now()\n",
    "        self.contents['tweets']={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        self.contents['searchTerm']=term\n",
    "        self.contents['time']=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet):\n",
    "        id = tweet['id_str']\n",
    "        tweets=self.contents['tweets']\n",
    "        if id not in tweets.keys():\n",
    "            tweets[id]={}\n",
    "            tweets[id]['tweet']=tweet\n",
    "            tweets[id]['count']=0\n",
    "        # if a count is not provided in the call, increment the count \n",
    "        if count == 0:\n",
    "            tweets[id]['count'] = tweets[id]['count'] +1\n",
    "        else:\n",
    "            tweets[id]['count'] = count\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        id = str(id)\n",
    "        tweets=self.contents['tweets']\n",
    "        if id in tweets:\n",
    "            return tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.contents['tweets'][id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.contents['tweets'])\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        tweets=self.contents['tweets']\n",
    "        for t,entry in tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.contents['tweets'].keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.contents)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        self.contents={}\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.contents=incontents\n",
    "        \n",
    "    def getSearchTerm(self):\n",
    "        return self.contents['searchTerm']\n",
    "     \n",
    "                \n",
    "    ### NEW ROUTINE - add a code to a tweet\n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "        \n",
    "    ### NEW ROUTINE  - add multiple  codes for a tweet\n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    "    ### NEW ROUTINE get codes for a tweet\n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this set up, we can reload the tweets from the file and reload the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @NBCNews: Police: Teen tried to detonate bomb at Utah school; backpack with device was found smoking; teen had been researching about IS…'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets =Tweets()\n",
    "tweets.readTweets(\"tweets-smoking.json\")\n",
    "ids=list(tweets.getIds())\n",
    "ids.sort()\n",
    "working=[]\n",
    "\n",
    "for i in range(20):\n",
    "    id = ids[i]\n",
    "    working.append(id)\n",
    "\n",
    "td = working[0]\n",
    "t = tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above we noted that this tweet was interesting becuase:\n",
    "1. it is a retweet\n",
    "2. It contains a user mention\n",
    "\n",
    "So, we will add codes to the appropriate tweet as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCode(td,\"RETWEET\")\n",
    "tweets.addCode(td,\"USERMENTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also confirm that this tweet is associated with the desired codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RETWEET', 'USERMENTION'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.getCodes(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see if this is a retweet by checking for the `retweeted_status` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'retweeted_status' in t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Let's look at the next tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @kushplug20: @Budfessions Smoking marijuana lowers your body temperature and makes you cool, Dm for orders and questions. #marijuana,#Ma…'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[1]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tweet is a retweet with a user mention and several hashtags. We might eventually be interested in stroing all hashtags, but for now, the most interesting aspect is likely that this tweet mentions marijuana. So, we add a code for `marijuana`.\n",
    "\n",
    "This time, we wil, for simplicity, use the `addCodes` routine to add the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,[\"RETWEET\", 'USERMENTION','HASHTAG','MARIJUANA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'retweeted_status' in t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok.. moving on to the third tweet.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@nytimes Why?\\nWe now have a statue of a crack smoking mayor in DC, what's the big deal?\\n#LiberalismIsAMentalDisorder\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[2]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tweet includes a user mention, a hashtag, and a mention of a specific drug (`crack`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','HASHTAG','CRACK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'retweeted_status' in t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @theblaze: The homemade bomb failed to go off and was discovered in a common area at the school by a student who reported a smoking back…'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[3]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this tweet talks about a `smoking back..` (probably a smoking backpack). As interesting as this might be in some sense, it is probably not relevant to discussions of inhaled smoke. Thus, we mark it as `IRRELEVANT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','IRRELEVANT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'retweeted_status' in t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @RAETHED0N: i love smoking weed. i hate advice.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[4]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This retweet includes a user mention, a reference to marijuana (`weed`), and a statement of positive affect (` i love smoking weeed.`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','MARIJUANA','POSAFFECT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @maciassalyssa: Smoking weed is a turn off\\U0001f927don’t @ me'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[5]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retweet with a user mention, a reference to marijuana, and a comment with negative affect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','MARIJUANA','NEGAFFECT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @usatodayvideo: An Indonesian zoo is taking heat after video surfaced of an orangutan smoking like a pro. https://t.co/d0fvCxzRn5'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[6]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','LINK','USERMENTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @NBCNews: Police: Teen tried to detonate bomb at Utah school; backpack with device was found smoking; teen had been researching about IS…'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[7]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tweet is similar to the above tweet  about the backpack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','IRRELEVANT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @mrsdianek: @FoxNews @realDonaldTrump Whatever you're smoking, send me some.  I need a break from reality too.\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[8]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','IRRELEVANT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone through several tweets, we can review the codes used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RETWEET', 'USERMENTION'}\n",
      "{'RETWEET', 'MARIJUANA', 'USERMENTION', 'HASHTAG'}\n",
      "{'RETWEET', 'CRACK', 'USERMENTION', 'HASHTAG'}\n",
      "{'RETWEET', 'IRRELEVANT', 'USERMENTION'}\n",
      "{'RETWEET', 'MARIJUANA', 'POSAFFECT', 'USERMENTION'}\n",
      "{'RETWEET', 'MARIJUANA', 'USERMENTION', 'NEGAFFECT'}\n",
      "{'RETWEET', 'LINK', 'USERMENTION'}\n",
      "{'RETWEET', 'IRRELEVANT', 'USERMENTION'}\n",
      "{'RETWEET', 'IRRELEVANT', 'USERMENTION'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    td=working[i]\n",
    "    print(tweets.getCodes(td))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having annotated several tweets, we might want to save the annotations in a file for future use. Fortnuately, the approach that we've used in our save and reload code is flexible enough to handle this without any further changes to the implementation. \n",
    "\n",
    "How does this work? The `Tweets` class stores all of the information abou the tweets in a simple dictionary. Tweet counts and codes are then stored inside the tweet object. When we go to save the set of Tweets, we simply turn this dictionary into JSON and then write it to a file. To read things in, we just read the JSON from the file and convert the result back into a dictionary. Thus, anything that we add to the dictionary will automatically be writen out and read back in.  We still need additional routines to access this data (like `addCode`, `addCodes`, and `getCodes`), but we  don't need to change the save/load routines.  Let's try it out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.saveTweets(\"tweets-smoking-annotated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets2=Tweets()\n",
    "tweets2.readTweets(\"tweets-smoking-annotated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @mrsdianek: @FoxNews @realDonaldTrump Whatever you're smoking, send me some.  I need a break from reality too.\n",
      "{'RETWEET', 'IRRELEVANT', 'USERMENTION'}\n",
      "RT @mrsdianek: @FoxNews @realDonaldTrump Whatever you're smoking, send me some.  I need a break from reality too.\n",
      "{'RETWEET', 'IRRELEVANT', 'USERMENTION'}\n"
     ]
    }
   ],
   "source": [
    "print(tweets.getTweet(td)['text'])\n",
    "print(tweets.getCodes(td))\n",
    "print(tweets2.getTweet(td)['text'])\n",
    "print(tweets2.getCodes(td))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "## Exercise 2.1: Examining the distribution of codes across a corpus\n",
    "\n",
    "Having annotated a number of tweets, you might want to get an idea of how many tweets are being used and how often. Write a new method inside the `Tweets` class tp will provide a list of codes and counts, sorted by frequency.  Please be sure to reload the tweets after you redefine the class.\n",
    "\n",
    "*ANSWER FOLLOWS - cut below here*\n",
    "Following lines to be deleted when provided for student use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.contents={}\n",
    "        self.contents['time']=datetime.now()\n",
    "        self.contents['tweets']={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        self.contents['searchTerm']=term\n",
    "        self.contents['time']=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet):\n",
    "        id = tweet['id_str']\n",
    "        tweets=self.contents['tweets']\n",
    "        if id not in tweets.keys():\n",
    "            tweets[id]={}\n",
    "            tweets[id]['tweet']=tweet\n",
    "            tweets[id]['count']=0\n",
    "        # if a count is not provided in the call, increment the count \n",
    "        if count == 0:\n",
    "            tweets[id]['count'] = tweets[id]['count'] +1\n",
    "        else:\n",
    "            tweets[id]['count'] = count\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        id = str(id)\n",
    "        tweets=self.contents['tweets']\n",
    "        if id in tweets:\n",
    "            return tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.contents['tweets'][id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.contents['tweets'])\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        tweets=self.contents['tweets']\n",
    "        for t,entry in tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.contents['tweets'].keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.contents)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        self.contents={}\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.contents=incontents\n",
    "        \n",
    "    def getSearchTerm(self):\n",
    "        return self.contents['searchTerm']\n",
    "     \n",
    "                \n",
    "    ### NEW ROUTINE - add a code to a tweet\n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "        \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        tweets=self.contents['tweets']\n",
    "        for id in tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets=Tweets()\n",
    "tweets.readTweets(\"tweets-smoking-annotated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VAPING', 1),\n",
       " ('USERMENTION', 17),\n",
       " ('RETWEET', 16),\n",
       " ('POSAFFECT', 2),\n",
       " ('NEGAFFECT', 2),\n",
       " ('MARIJUANA', 5),\n",
       " ('LINK', 4),\n",
       " ('IRRELEVANT', 5),\n",
       " ('HASHTAG', 3),\n",
       " ('CRACK', 1)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.getCodeProfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END CUT HERE*\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## Exercise 2.2: Code the Next 10 tweets in the set. \n",
    "Start with the tags used above, adding your own as needed.  Code up to and including the tweet  with index 20 in the `working` array. Examine the code profile and save your tweets  to a new file when you are done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER FOLLOWS - cut below here*\n",
    "Following lines to be deleted when provided for student use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @ohPudu: Rare colorized photo of Virgin Mary smoking and Jesus. https://t.co/Z9Pr6t4dBn'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids=list(tweets.getIds())\n",
    "ids.sort()\n",
    "working=[]\n",
    "\n",
    "for i in range(9,19):\n",
    "    id = ids[i]\n",
    "    working.append(id)\n",
    "\n",
    "td = working[0]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','LINK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @BuzzFeedNews: A teenage boy who allegedly spray painted a message about ISIS on a Southern Utah high school, has been arrested for leav…'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[1]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','IRRELEVANT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @JWilltize: @RMadridBabe I think that smoke was coming from whatever Xavi was smoking'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[2]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RT @stoner_stuff: DON'T EVER APOLOGIZE for SMOKING WEED #WednesdayWisdom\""
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[3]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','MARIJUANA','POSAFFECT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Sporf What is he smoking?'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[4]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['USERMENTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @shujaxhaider: the guy sitting next to me on the bus is watching a video of himself smoking weed'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[5]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','MARIJUANA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Stop smoking, we love you\\nAnd we don't want you to die\\n#CarSeatHeadrest #StopSmoking https://t.co/mIQvVzAWTD\""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[6]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['HASHTAG','LINK','NEGAFFECT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using household cleaning products could be as bad for you as smoking https://t.co/UlYvTqJQZZ https://t.co/XKv6ahNUI6'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[7]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['LINK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @aubreyrutten: Vaping and smoking are SO unattractive, idk how people like that tbh'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[8]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','VAPING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @AndrewCollins: I am sad, but not surprised, that the @NME has stopped the presses. I remain deeply proud of having been associated with…'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = working[9]\n",
    "t=tweets.getTweet(td)\n",
    "t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.addCodes(td,['RETWEET','USERMENTION','IRRELEVANT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VAPING', 1),\n",
       " ('USERMENTION', 17),\n",
       " ('RETWEET', 16),\n",
       " ('POSAFFECT', 2),\n",
       " ('NEGAFFECT', 2),\n",
       " ('MARIJUANA', 5),\n",
       " ('LINK', 4),\n",
       " ('IRRELEVANT', 5),\n",
       " ('HASHTAG', 3),\n",
       " ('CRACK', 1)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.getCodeProfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.saveTweets(\"tweets-smoking-annotated.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END CUT*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXERCISE 2.3: Reflection on coding\n",
    "\n",
    "Open coding can often be an iterative process. When we first start out, we don't really know what we're looking for. As a result, the first few items annotated might only get a few codes, and we might miss ideas that we don't initially think are important. As we see more and more items, our ideas of what needs to be annotated will change, and we'll start adding in codes that might also apply to earlier messages. Thus, we often need to review and re-annotate earlier tweets to account for changes in our interpreations.\n",
    "\n",
    "Review your annotations the tweets that you reviewed. Revise the codes associated with these tweets, adding items from the overall list of codes as appropriate. Describe the change that you have made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS - cut below here*\n",
    "Following lines to be deleted when provided for student use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*END CUT*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2.4: Reflection on storage/serialization\n",
    "\n",
    "In working with this small set of 100 tweets, we are taking a very simple approach to storage and management of the tweets and annotations. Storing everything in a nested Python dictionary and then dumping it to disk as JSON text can be very appealing. What are the strengths and weaknesses of this approach, and how might these strengths and weaknesses differ with larger datasets containing 100,000 or 100 million datasets? What alternative  strategies might you use for larger datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS - cut below here*\n",
    "Following lines to be deleted when provided for student use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Advantages*: JSON is easy to read, as programmers can open the text file and read contents directly. JSON is also easy to work with from multiple programming languages and platforms, allowing a collection of tweets written in Python to be read by code written in other languages.  Finally, the JSON structure is easily adaptable, allowing fields to be easily added or removed as needed.\n",
    "\n",
    "*Disadvantages*: As a text-based format, JSON data is pretty-much \"all or nothing\". Although it might be possible to read in part of the JSON structure, this will complicate code significantly.  Thus, most programs would read the entire JSON structure into data all at once. This is fine for small datasets, but might get slow and bulky, requiring lots of RAM, for larger datasets. \n",
    "\n",
    "Very large sets of tweets might be stored in a database. A relational database might be created to store tweets in one or more tables, providing the power of the structured query language (SQL) to retrieve tweets matching only specified criteria.  SQL could also be used to quickly and easily calculate aggregate statistics, without loading all of the tweets into RAM. A downside of this approach is the need to manage a distinct software component (the database server), the difficulty in changing the contents of the database, the complexity of SQL queries, and the relatively complex and often language-specific tools needed to manage the connections to the database. \n",
    "\n",
    "Alternatively, NoSQL databases that work very well with JSON might be considered. These databases may share some of the challenges associated with relational databases, but they are often also more flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*END CUT*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Final Notes\n",
    "\n",
    "Now that we have completed the initial annotation, you can move on to [Part 3](SocialMedia - Part 3.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
