{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media and Human-Computer Interaction - Part 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *Goal*: Use social media posts to explore the appplication of text and natural language processing to see what might be learned from online interactions.\n",
    "\n",
    "Specifically, we will retrieve, annotate, process, and interpret Twitter data on health-related issues such as depression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "References:\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n",
    "* The [Tweepy Python API for Twitter](http://www.tweepy.org/)\n",
    "\n",
    "Required Software\n",
    "* [Python 3](https://www.python.org)\n",
    "* [NumPy](http://www.numpy.org) - for preparing data for plotting\n",
    "* [Matplotlib](https://matplotlib.org) - plots and garphs\n",
    "* [jsonpickle](https://jsonpickle.github.io) for storing tweets. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This module continues the Social Media Data Science module started in [Part 1](SocialMedia - Part 1.ipynb), covering the natural language processing analysis of our tweet corpus, including \n",
    "\n",
    "  1. Natural Language Processings\n",
    "  2. Construction of classifiers\n",
    "  \n",
    "Our case study will apply these topics to Twitter discussions of smoking and vaping. Although details of the tools used to access data and the format and content of the data may differ for various services, the strategies and procedures used to analyze the data will generalize to other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Before we dig in, we must grab a bit of code from [Part 1](SocialMedia - Part 1.ipynb):\n",
    "\n",
    "1. Our Tweets class\n",
    "3. Our twitter API Keys - be sure to copy the keys that you generated when you completed [Part 1](SocialMedia - Part 1.ipynb).\n",
    "4. Configuration of our Twitter connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*REDACT FOLLOWING DETAILS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key='D2L4YZ2YrO1PMix7uKUK63b8H'\n",
    "consumer_secret='losRw9T8zb6VT3TEJ9JHmmhAmn1GXKVj30dkiMv9vjhXuiWek9'\n",
    "access_token='15283934-iggs1hiZAPI2o5sfHWMfjumTF7SvytHPjpPRGf3I6'\n",
    "access_secret='bOvqssxS97PGPwXHQZxk83KtAcDyLhRLgdQaokCdVvwFi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Examination of text patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our ultimate goal is to build a classifier capable of distinguishing tweets related to tobacco smoing from other, unrelated tweets. To do this, we will eventually build natural language processing models. However, we will start by doing some basic processing to explore the types of words and language found in the tweets. \n",
    "\n",
    "To do this, we will use the [Spacy](https://spacy.io/) Python NLP package. Spacy provides significant NLP power out-of-the box, with customization facilities offering greater flexibility at various stages of the Pipeline. Details can be found at the  [Spacy web site](https://spacy.io/), and in this [tutorial](https://nicschrading.com/project/Intro-to-NLP-with-spaCy/).\n",
    "\n",
    "However, before we get into the deails, a bit of a roadmap. \n",
    "\n",
    "Natural Language Processing involves a series of operations on an input text, each building off of the previous step to add additional insight and undertanding.  Thus, many NLP packages run as pipeline processors providing modular components at each stage of the process. Separating key steps into discrete packages provides needed modularity, as developers can modify and customize individual components as needed. Spacy, like other NLP tools including [GATE](https://gate.ac.uk/) and [cTAKES](https://ctaes.apache.org)  operate on such a model. Although the specific components of each pipeline vary from system to system (and from tasks to task, the key tasks are rougly similar:\n",
    "\n",
    "1. *Tokenizing*: splitting the text into words, punctuation, and other markers.\n",
    "2. *Part of speech tagging*: Classifying terms as nouns, verbs, adjective, adverbs, ec.\n",
    "3. *Dependency Parsing* or *Chunking*: Defining relationships between tokens (subject and object of sentence) and grouping into noun and veb phrases.\n",
    "4. *Named Entity Recognition*: Mapping words or phrases to standard vocabularies or other common, known values. This step is often key for linking free text to accepted terms for diseases, symptoms, and/or anatomic locations.\n",
    "\n",
    "Each of these steps might be accomplished through rules, machine learning models, or some combination of approaches. After these initial steps are complete, results might be used to identify relationships between items in the text, build classifiers, or otherwise conduct further analysis. We'll get into these topics later.\n",
    "\n",
    "The [Spacy documentation](https://spacy.io/usage/spacy-101) and [cTAKES default pipeline description](https://cwiki.apache.org/confluence/display/CTAKES/Default+Clinical+Pipeline) provide two examples of how these components might be arranged in practice.  For more information on NLP theory and methods, see [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/), perhaps the leading NLP textbook.\n",
    "\n",
    "Given this introduction, we can read in our tweets and get to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 Reading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of [Part 2](SocialMedia - Part 2.ipynb) you had saved two sets of tweets one for smoking and one for vaping. Let's read  in the vaping twets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaping=Tweets()\n",
    "vaping.readTweets(\"tweets-vaping.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the smoking tweets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets-smoking.json\")\n",
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we will grab a specifc pre-chosen tweet and process it.  \n",
    "\n",
    "This will give us a beginning feel for what [Spacy](https://spacy.io) can do and how we might use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id='974316984740429824'\n",
    "sample=smoking.getText(tweet_id)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets have usage patterns that are non-standard English - URLs, hashtags, user references (this particularly tweet was not selected accidentally). These patterns create challenges for extracting content - we might want to know that \"#QuitSmoking\" is, in a tweet, a hashtag that should be considered as a complete unit.  \n",
    "\n",
    "We'll see soon how we might do this, but first, to start the NLP process, we can import the Spacy components and create an NLP object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then parse out the text from the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = nlp(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list of tokens. We can print out each token to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Smoking', 'affects', 'multiple', 'parts', 'of', 'our', 'body', '.', 'Know', 'more', ':', 'https://t.co/hwTeRdC9Hf', '\\n', '#', 'SwasthaBharat', '#', 'NHPIndia', '#', 'mCessation', '#', 'QuitSmoking', 'https://t.co/x7xHO9G2Cr']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see right away that this parsing isn't quite what we would like. Default English parsing treats  `#QuitSmoking`  as two separate tokens - `#` and `QuitSmoking`. To treat this as a hashtag, we will indeed need to revise the tokenizr - the component of an NLP system responsible for splitting text into tokens'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parsing and Part-Of-Speech Tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps in NLP involve *part of speech tagging* and *dependency parsing*. \n",
    "\n",
    "Part of speech tagging is the process of classifying each toekn as one of the parts of speech that we all learned in elementrary school. Parts of speech are assigned to attributes of each token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affects\n",
      "99\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(parsed[2].text)\n",
    "print(parsed[2].pos)\n",
    "print(parsed[2].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have two attributes here - `pos` is the hash code for the part of speech, used for efficiency, while `pos_` is the human readable form. Other attributes derived by Spacy follow the same pattern.\n",
    "\n",
    "A second attribute - `tag` - provide es additional information.\n",
    "\n",
    "As described in the [Spacy documentation for part-of-speech tags](https://spacy.io/api/annotation#pos-tagging), the tags associated with these two fields come from different sources. 'tag_' uses parts-of-speech from a version of the [Penn Treebank](https://www.seas.upenn.edu/~pdtb/), a well-known corpus of annotated text. 'pos_' uses a simpler set of tags from [A Universal Part-of-Speech Tagset](https://arxiv.org/abs/1104.2086), published by researchers from Google.  \n",
    "\n",
    "The tags for `affects` provide an example of the difference. According to the [Spacy documentation ](https://spacy.io/api/annotation#pos-tagging) `VBZ` from the Penn tag set indicates a 'verb, 3rd person singular present', while 'the 'VERB' result for 'pos_' is a more general tag from the Google set. There are many types of verbs in the Penn Treebank that correspond tot the 'VERB' tag from the Google set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affects\n",
      "VBZ\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(parsed[2].text)\n",
    "print(parsed[2].tag_)\n",
    "print(parsed[2].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about a part of spech tag, you can use `spacy.explain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb\n",
      "verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(parsed[2].pos_))\n",
    "print(spacy.explain(parsed[2].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Chief among this is the *lemma_*: the \"standard\" or \"base\" form, reducing verb forms to their base verb, plurals to appropriate singular nouns, etc.  For example, the 29th token is `affects`, which has `affect` as the lemmatized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affects\n",
      "affect\n"
     ]
    }
   ],
   "source": [
    "tweet_id='974316984740429824'\n",
    "sample=smoking.getText(tweet_id)\n",
    "parsed=nlp(sample)\n",
    "print(parsed[2].text)\n",
    "print(parsed[2].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Spacy stores many fields as both hashes for efficiency and as text  for readability. You'll want to use the text form for interpreting results, but the hash for computing. They differ only in the use of the trailing underscore - thus `lemma` is the hash while `lemma_` is the human readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17543419487618836897\n",
      "affect\n"
     ]
    }
   ],
   "source": [
    "print(parsed[2].lemma)\n",
    "print(parsed[2].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other values returned by the parser might also be of interest:\n",
    "* is_stop is True if the token is a \"stop\" word - a commonly found word that might addd litle or no information.\n",
    "* is_alpha is True if the token is alphanumeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at token 1 (\"Smoking\"), token 4 (\"parts\"), token 12 (\"https://t.co/hwTeRdC9Hf'\"),  and token 14(\"#\") to see a few more tokens in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoking smoking NOUN NN False True\n",
      "parts part NOUN NNS False True\n",
      "https://t.co/hwTeRdC9Hf https://t.co/hwterdc9hf PROPN NNP False False\n",
      "# # PROPN NNP False False\n"
     ]
    }
   ],
   "source": [
    "t1 = parsed[1]\n",
    "t4 = parsed[4]\n",
    "t12= parsed[12]\n",
    "t14 = parsed[14]\n",
    "print (t1.text,t1.lemma_,t1.pos_,t1.tag_,t1.is_stop,t1.is_alpha)\n",
    "print (t4.text,t4.lemma_,t4.pos_,t4.tag_,t4.is_stop,t4.is_alpha)\n",
    "print (t12.text,t12.lemma_,t12.pos_,t12.tag_,t12.is_stop,t12.is_alpha)\n",
    "print (t14.text,t14.lemma_,t14.pos_,t14.tag_,t14.is_stop,t14.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that URLS are neither alphabetical  nor stop-words, but they are proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some NLP systems will go a bit further than Spacy's lemmatization, using a process called \"stemming\" to reduce words to base forms. With a stemming algorithm, \"scared\" might be reduced to \"scare\" - see this description of [Porter's stemming algorithm](https://tartarus.org/martin/PorterStemmer/) for more detail. \n",
    "\n",
    "Let's turn the code that we used above into a routine, along with a routine to print out token details and try another tweet or two. To make things easy to read, we'll use some spaces to format things in columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTweetText(tweets):\n",
    "    tweet_id=random.choice(list(tweets.getIds()))\n",
    "    return tweets.getText(tweet_id)\n",
    "\n",
    "def printTokDetails(parsed):\n",
    "    print(\"{:25} {:25} {:7}{:7}{:7}{:7}\".format(\"Token text\",\"Lemma\",\"POS\",\"Tag\",\"Stop?\",\"Alpha?\"))\n",
    "    for tok in parsed:\n",
    "        print(\"{:25} {:25} {:7}{:7}{:7}{:7}\".format(str(tok.text),str(tok.lemma_),str(tok.pos_),str(tok.tag_),str(tok.is_stop),str(tok.is_alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2=getTweetText(smoking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seeing kids smoking in school uniform makes me so angry!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed2=nlp(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text                Lemma                     POS    Tag    Stop?  Alpha? \n",
      "Seeing                    see                       VERB   VBG    False  True   \n",
      "kids                      kid                       NOUN   NNS    False  True   \n",
      "smoking                   smoke                     VERB   VBG    False  True   \n",
      "in                        in                        ADP    IN     False  True   \n",
      "school                    school                    NOUN   NN     False  True   \n",
      "uniform                   uniform                   NOUN   NN     False  True   \n",
      "makes                     make                      VERB   VBZ    False  True   \n",
      "me                        -PRON-                    PRON   PRP    False  True   \n",
      "so                        so                        ADV    RB     False  True   \n",
      "angry                     angry                     ADJ    JJ     False  True   \n",
      "!                         !                         PUNCT  .      False  False  \n"
     ]
    }
   ],
   "source": [
    "printTokDetails(parsed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see some interesting pattners arising here.  For example:\n",
    "\n",
    "* We see many different type of speech. Initially, we might want to focus on the nouns alone, as they provide much of the content.  \n",
    "\n",
    "* Look for words like \"is\" or \"was\" - these might all refer to a common lemma term - \"be\", corresponding to the generic form of he verb. Do you see any other incidents of lemma forms that differ from the parsed text?\n",
    "\n",
    "* URLs and icons might be present in tweets. Are they classified as alphanumeric? Should we include them as part of the \"useful\" text from a tweet? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## EXERCISE 3.1: Filtering tokens\n",
    "\n",
    "Although NLP parsing is often a good start, further filtering is often necessary to focus on data relevant for specific tasks. In this problem, we will review some additional tweets and develop a post-processing routine capable of filtering tweets as necessary for our needs. \n",
    "\n",
    "3.1.1 Using the `getTweetText`, and `printTokDetails` routines above, aong with the spacy `parser` command, examine several tweets to decide which tokens should be included or not.  List criteria for keeeping/removing tokens. Remember to use `spacy.explain()` for any unfamiliar POS or tag entries. Note that your  criteria will not be perfect, and will likely need refinining. Examiine enough tweets to feel confident in your criteria.\n",
    "\n",
    "3.1.2 Write a routine  `includeToken` that will return True if a token matches the criteria that you identified in 3.11, and false otherwise. Assume for now that we are only interested in nouns and verbs, as they might be a good starting point to find information about vaping or smoking. \n",
    "\n",
    "3.1.3 Write a routine `filterTweetTokens` that will filter the parsed tokens from a single tweet, returning a list of the tokens to be included, based on your criteria.\n",
    "\n",
    "3.1.4 Run `filterTweetTokens` on a few tweets. Identify any inaccuracies and explain them. When possible, identify an approach for improving performance, and implement it in a revision version of `filterTweetTokens`.\n",
    "\n",
    "3.1.5. Add these routines to the tweet class, along with some new routines.\n",
    "\n",
    "3.1.5.1 `parseTweet` will parse one of the tweets in the collection, storing the full list of tokens will be stored in a new entry in the dictionary entitled 'tokens'. `parseTweet` will also filter the tweets, storing the resulting list in an entry entitled 'filteredTokens'.\n",
    "\n",
    "*NOTE*: The tweets class might or might not have an NLP object available for any given call to `parseTweet`. You should have the class create an NLP object when it is initialzed. \n",
    "\n",
    "3.1.5.2 `parseTweets` will call `parseTweet` on all of the tweets in a collection.\n",
    "\n",
    "3.1.5.3 `getTokens` will be used to get all of the tokens for a given tweet.\n",
    "\n",
    "3.1.5.4 `getFilteredTokens` will be used to get all of the filtered tokens for a tweet. \n",
    "\n",
    "\n",
    "3.1.6 When you are done, test this new version of the class by reading in and parsing the 'smoking' tweet set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS Cut below here*\n",
    "\n",
    "### 3.1.1 Sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made a sandwich 10 min ago and been looking for it ever since then🤦🏾‍♂️ I gotta stop smoking😂 https://t.co/NCbNOyvZXe\n",
      "Token text                Lemma                     POS    Tag    Stop?  Alpha? \n",
      "Made                      make                      VERB   VBN    False  True   \n",
      "a                         a                         DET    DT     False  True   \n",
      "sandwich                  sandwich                  NOUN   NN     False  True   \n",
      "10                        10                        NUM    CD     False  False  \n",
      "min                       min                       NOUN   NN     False  True   \n",
      "ago                       ago                       ADV    RB     False  True   \n",
      "and                       and                       CCONJ  CC     False  True   \n",
      "been                      be                        VERB   VBN    False  True   \n",
      "looking                   look                      VERB   VBG    False  True   \n",
      "for                       for                       ADP    IN     False  True   \n",
      "it                        -PRON-                    PRON   PRP    False  True   \n",
      "ever                      ever                      ADV    RB     False  True   \n",
      "since                     since                     ADP    IN     False  True   \n",
      "then                      then                      ADV    RB     False  True   \n",
      "🤦                         🤦                         ADJ    JJ     False  False  \n",
      "🏾‍                        🏾‍                        PROPN  NNP    False  False  \n",
      "♂                         ♂                         PROPN  NNP    False  False  \n",
      "️                         ️                         NOUN   NN     False  False  \n",
      "I                         -PRON-                    PRON   PRP    False  True   \n",
      "got                       get                       VERB   VBD    False  True   \n",
      "ta                        to                        PART   TO     False  True   \n",
      "stop                      stop                      VERB   VB     False  True   \n",
      "smoking                   smoking                   NOUN   NN     False  True   \n",
      "😂                         😂                         NOUN   NNS    False  False  \n",
      "https://t.co/NCbNOyvZXe   https://t.co/ncbnoyvzxe   PUNCT  .      False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"'Weak' evidence linking e-cigarette use with future smoking\"\n",
      "\n",
      "Using e-cigarettes is a recommended way of giving up smoking, but can their use make children more likely to take up the real thing? @nhschoices looks at the evidence\n",
      "\n",
      "https://t.co/Mh22zscOBI\n",
      "\n",
      "#behindtheheadlines https://t.co/3h5FetrYWi\n",
      "Token text                Lemma                     POS    Tag    Stop?  Alpha? \n",
      "\"                         \"                         PUNCT  ``     False  False  \n",
      "'                         '                         PUNCT  ``     False  False  \n",
      "Weak                      weak                      ADJ    JJ     False  True   \n",
      "'                         '                         PUNCT  ''     False  False  \n",
      "evidence                  evidence                  NOUN   NN     False  True   \n",
      "linking                   link                      VERB   VBG    False  True   \n",
      "e                         e                         NOUN   NN     False  True   \n",
      "-                         -                         PUNCT  HYPH   False  False  \n",
      "cigarette                 cigarette                 NOUN   NN     False  True   \n",
      "use                       use                       NOUN   NN     False  True   \n",
      "with                      with                      ADP    IN     False  True   \n",
      "future                    future                    ADJ    JJ     False  True   \n",
      "smoking                   smoking                   NOUN   NN     False  True   \n",
      "\"                         \"                         PUNCT  ''     False  False  \n",
      "\n",
      "\n",
      "                        \n",
      "\n",
      "                        SPACE  _SP    False  False  \n",
      "Using                     use                       VERB   VBG    False  True   \n",
      "e                         e                         NOUN   NN     False  True   \n",
      "-                         -                         PUNCT  HYPH   False  False  \n",
      "cigarettes                cigarette                 NOUN   NNS    False  True   \n",
      "is                        be                        VERB   VBZ    False  True   \n",
      "a                         a                         DET    DT     False  True   \n",
      "recommended               recommend                 VERB   VBN    False  True   \n",
      "way                       way                       NOUN   NN     False  True   \n",
      "of                        of                        ADP    IN     False  True   \n",
      "giving                    give                      VERB   VBG    False  True   \n",
      "up                        up                        PART   RP     False  True   \n",
      "smoking                   smoking                   NOUN   NN     False  True   \n",
      ",                         ,                         PUNCT  ,      False  False  \n",
      "but                       but                       CCONJ  CC     False  True   \n",
      "can                       can                       VERB   MD     False  True   \n",
      "their                     -PRON-                    ADJ    PRP$   False  True   \n",
      "use                       use                       VERB   VB     False  True   \n",
      "make                      make                      VERB   VB     False  True   \n",
      "children                  child                     NOUN   NNS    False  True   \n",
      "more                      more                      ADV    RBR    False  True   \n",
      "likely                    likely                    ADJ    JJ     False  True   \n",
      "to                        to                        PART   TO     False  True   \n",
      "take                      take                      VERB   VB     False  True   \n",
      "up                        up                        PART   RP     False  True   \n",
      "the                       the                       DET    DT     False  True   \n",
      "real                      real                      ADJ    JJ     False  True   \n",
      "thing                     thing                     NOUN   NN     False  True   \n",
      "?                         ?                         PUNCT  .      False  False  \n",
      "@nhschoices               @nhschoice                NOUN   NNS    False  False  \n",
      "looks                     look                      VERB   VBZ    False  True   \n",
      "at                        at                        ADP    IN     False  True   \n",
      "the                       the                       DET    DT     False  True   \n",
      "evidence                  evidence                  NOUN   NN     False  True   \n",
      "\n",
      "\n",
      "                        \n",
      "\n",
      "                        SPACE  _SP    False  False  \n",
      "https://t.co/Mh22zscOBI   https://t.co/mh22zscobi   PRON   PRP    False  False  \n",
      "\n",
      "\n",
      "                        \n",
      "\n",
      "                        SPACE  _SP    False  False  \n",
      "#                         #                         SYM    $      False  False  \n",
      "behindtheheadlines        behindtheheadline         NOUN   NNS    False  True   \n",
      "https://t.co/3h5FetrYWi   https://t.co/3h5fetrywi   X      ADD    False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love smoking weed in beautiful ass places, looking at beautiful ass things.\n",
      "Token text                Lemma                     POS    Tag    Stop?  Alpha? \n",
      "I                         -PRON-                    PRON   PRP    False  True   \n",
      "love                      love                      VERB   VBP    False  True   \n",
      "smoking                   smoke                     VERB   VBG    False  True   \n",
      "weed                      weed                      NOUN   NN     False  True   \n",
      "in                        in                        ADP    IN     False  True   \n",
      "beautiful                 beautiful                 ADJ    JJ     False  True   \n",
      "ass                       ass                       NOUN   NN     False  True   \n",
      "places                    place                     NOUN   NNS    False  True   \n",
      ",                         ,                         PUNCT  ,      False  False  \n",
      "looking                   look                      VERB   VBG    False  True   \n",
      "at                        at                        ADP    IN     False  True   \n",
      "beautiful                 beautiful                 ADJ    JJ     False  True   \n",
      "ass                       ass                       NOUN   NN     False  True   \n",
      "things                    thing                     NOUN   NNS    False  True   \n",
      ".                         .                         PUNCT  .      False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is full of forgotten heroines, but my favorite might be the Québécois woman who hired a hearse so that she could be driven around town smoking in the coffin-bed while enjoying the view. https://t.co/TxjFEO25lR\n",
      "Token text                Lemma                     POS    Tag    Stop?  Alpha? \n",
      "History                   history                   NOUN   NN     False  True   \n",
      "is                        be                        VERB   VBZ    False  True   \n",
      "full                      full                      ADJ    JJ     False  True   \n",
      "of                        of                        ADP    IN     False  True   \n",
      "forgotten                 forget                    VERB   VBN    False  True   \n",
      "heroines                  heroine                   NOUN   NNS    False  True   \n",
      ",                         ,                         PUNCT  ,      False  False  \n",
      "but                       but                       CCONJ  CC     False  True   \n",
      "my                        -PRON-                    ADJ    PRP$   False  True   \n",
      "favorite                  favorite                  NOUN   NN     False  True   \n",
      "might                     may                       VERB   MD     False  True   \n",
      "be                        be                        VERB   VB     False  True   \n",
      "the                       the                       DET    DT     False  True   \n",
      "Québécois                 québécois                 ADJ    JJ     False  True   \n",
      "woman                     woman                     NOUN   NN     False  True   \n",
      "who                       who                       NOUN   WP     False  True   \n",
      "hired                     hire                      VERB   VBD    False  True   \n",
      "a                         a                         DET    DT     False  True   \n",
      "hearse                    hearse                    NOUN   NN     False  True   \n",
      "so                        so                        ADP    IN     False  True   \n",
      "that                      that                      ADP    IN     False  True   \n",
      "she                       -PRON-                    PRON   PRP    False  True   \n",
      "could                     could                     VERB   MD     False  True   \n",
      "be                        be                        VERB   VB     False  True   \n",
      "driven                    drive                     VERB   VBN    False  True   \n",
      "around                    around                    ADP    IN     False  True   \n",
      "town                      town                      NOUN   NN     False  True   \n",
      "smoking                   smoking                   NOUN   NN     False  True   \n",
      "in                        in                        ADP    IN     False  True   \n",
      "the                       the                       DET    DT     False  True   \n",
      "coffin                    coffin                    NOUN   NN     False  True   \n",
      "-                         -                         PUNCT  HYPH   False  False  \n",
      "bed                       bed                       NOUN   NN     False  True   \n",
      "while                     while                     ADP    IN     False  True   \n",
      "enjoying                  enjoy                     VERB   VBG    False  True   \n",
      "the                       the                       DET    DT     False  True   \n",
      "view                      view                      NOUN   NN     False  True   \n",
      ".                         .                         PUNCT  .      False  False  \n",
      "https://t.co/TxjFEO25lR   https://t.co/txjfeo25lr   X      XX     False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria: \n",
    "    \n",
    "* Alpha is true, and \n",
    "* Stop is false, and \n",
    "* text is not \"RT\"\n",
    "* Tag is NN, Tag is NNP, or POS is VERB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2  `includeToken`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our routine will accept a token only if it meets the criteria given above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_alpha == True and tok.is_stop == False:\n",
    "        if tok.text =='RT':\n",
    "            val = False\n",
    "        elif tok.pos_=='NOUN' or tok.pos_=='VERB':\n",
    "            val = True\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made a sandwich 10 min ago and been looking for it ever since then🤦🏾‍♂️ I gotta stop smoking😂 https://t.co/NCbNOyvZXe\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[0])\n",
    "includeToken(parsed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[1])\n",
    "includeToken(parsed[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sandwich\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[2])\n",
    "includeToken(parsed[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made True\n",
      "a False\n",
      "sandwich True\n",
      "10 False\n",
      "min True\n",
      "ago False\n",
      "and False\n",
      "been True\n",
      "looking True\n",
      "for False\n",
      "it False\n",
      "ever False\n",
      "since False\n",
      "then False\n",
      "🤦 False\n",
      "🏾‍ False\n",
      "♂ False\n",
      "️ False\n",
      "I False\n",
      "got True\n",
      "ta False\n",
      "stop True\n",
      "smoking True\n",
      "😂 False\n",
      "https://t.co/NCbNOyvZXe False\n"
     ]
    }
   ],
   "source": [
    "for tok in parsed:\n",
    "    print(tok,includeToken(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looks ok. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Write a routine `filterTweetTokens` that will parse a single tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for tok in tokens:\n",
    "        if includeToken(tok) == True:\n",
    "            filtered.append(tok)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made a sandwich 10 min ago and been looking for it ever since then🤦🏾‍♂️ I gotta stop smoking😂 https://t.co/NCbNOyvZXe\n",
      "Made\n",
      "sandwich\n",
      "min\n",
      "been\n",
      "looking\n",
      "got\n",
      "stop\n",
      "smoking\n"
     ]
    }
   ],
   "source": [
    "f= filterTweetTokens(parsed)\n",
    "print(sample)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Run `filterTweetTokens` on a few tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level of saltiness: When people post videos of them smoking weed and you watch them just suck it in and blow it out right after\n",
      "Level\n",
      "saltiness\n",
      "people\n",
      "post\n",
      "videos\n",
      "smoking\n",
      "weed\n",
      "watch\n",
      "suck\n",
      "blow\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Wyd after smoking this?\" https://t.co/nNGtlDRS95\n",
      "Wyd\n",
      "smoking\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level of saltiness: When people post videos of them smoking weed and you watch them just suck it in and blow it out right after\n",
      "Level\n",
      "saltiness\n",
      "people\n",
      "post\n",
      "videos\n",
      "smoking\n",
      "weed\n",
      "watch\n",
      "suck\n",
      "blow\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "girls smoking hippy eve rapper nude pics https://t.co/HJXd3NXdKL\n",
      "girls\n",
      "smoking\n",
      "eve\n",
      "rapper\n",
      "pics\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now more ex-smokers than smokers across @WYHpartnership We want more! @yorkshirecancer statement published today backs use of e-cigarettes, combined with specialist stop smoking advice from @YSmokefree and others, as best way to quit. @breathe2025 @PHE_uk https://t.co/nqdrYB17Cw\n",
      "are\n",
      "smokers\n",
      "smokers\n",
      "want\n",
      "statement\n",
      "published\n",
      "today\n",
      "backs\n",
      "use\n",
      "e\n",
      "cigarettes\n",
      "combined\n",
      "specialist\n",
      "stop\n",
      "smoking\n",
      "advice\n",
      "others\n",
      "way\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoking can interrupt blood flow to the brain and may lead to stroke via @FDATobacco #BrainWeek https://t.co/hSI1eeTEeP\n",
      "Smoking\n",
      "can\n",
      "interrupt\n",
      "blood\n",
      "flow\n",
      "brain\n",
      "may\n",
      "lead\n",
      "stroke\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5  Adding to the Tweets class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        self.nlp = spacy.load('en')\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "  \n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary\n",
    "    \n",
    "    # new routine for classifying a token\n",
    "    def includeToken(self,tok):\n",
    "        val =False\n",
    "        if tok.is_alpha == True and tok.is_stop == False:\n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        return val\n",
    "    \n",
    "    # new routine for filtering a list of tokens.\n",
    "    def filterTweetTokens(self,tokens):\n",
    "        filtered=[]\n",
    "        for tok in tokens:\n",
    "            if includeToken(tok) == True:\n",
    "                filtered.append(tok)\n",
    "        return filtered\n",
    "    \n",
    "    def parseTweet(self,id):\n",
    "        text = self.getText(id)\n",
    "        parsed = nlp(text)\n",
    "        self.tweets[id]['tokens']=parsed\n",
    "        filtered= self.filterTweetTokens(parsed)\n",
    "        self.tweets[id]['filteredTokens']=filtered\n",
    "        \n",
    "    def parseTweets(self):\n",
    "        ids=self.getIds()\n",
    "        for id in ids:\n",
    "            self.parseTweet(id)\n",
    "            \n",
    "    def getTokens(self,id):\n",
    "        if 'tokens' in self.tweets[id]:\n",
    "            return self.tweets[id]['tokens']\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "    def getFilteredTokens(self,id):\n",
    "        if 'filteredTokens' in self.tweets[id]:\n",
    "             return self.tweets[id]['filteredTokens']\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Trying out the new routines for parsing a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets-smoking.json\")\n",
    "smoking.parseTweets()\n",
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Smoking', 'a', 'joint', 'is', 'always', 'a', 'lil', 'date', 'w', 'Christian', ',', 'but', 'smoking', 'a', 'joint', 'in', 'the', 'hot', 'tub', 'that', 'was', 'NIGHT', 'OUT', 'HA']\n",
      "['Smoking', 'joint', 'is', 'lil', 'date', 'smoking', 'joint', 'tub', 'was']\n"
     ]
    }
   ],
   "source": [
    "tweet_id=random.choice(list(smoking.getIds()))\n",
    "smoking.getText(tweet_id)\n",
    "toks = smoking.getTokens(tweet_id)\n",
    "print([token.text for token in toks])\n",
    "filtered = smoking.getFilteredTokens(tweet_id)\n",
    "print([f.text for f in filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', ':', 'smoking', 'weed', 'has', 'n’t', 'affected', 'me', 'at', 'all', '\\n\\n', 'someone', ':', 'count', 'to', '10', '\\n\\n', 'me', ':', 'https://t.co/SUoGzARpom']\n",
      "['smoking', 'weed', 'has', 'affected', 'someone', 'count']\n"
     ]
    }
   ],
   "source": [
    "tweet_id=random.choice(list(smoking.getIds()))\n",
    "smoking.getText(tweet_id)\n",
    "toks = smoking.getTokens(tweet_id)\n",
    "print([token.text for token in toks])\n",
    "filtered = smoking.getFilteredTokens(tweet_id)\n",
    "print([f.text for f in filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'UNFAO', 'is', 'scaling', 'up', 'efforts', 'on', 'reducing', 'the', 'amount', 'of', '#', 'wood', 'used', 'as', '#', 'fuel', 'for', '#', 'fish', 'smoking', 'in', 'the', '#', 'Gambia', '.', 'With', 'the', 'new', '#', 'UNFAO', 'Thiaroye', 'Technology', 'stove', ',', '#', 'women', 'in', 'the', '#', 'fishsmoking', '&', 'amp', ';', 'drying', '#', 'industry', 'have', 'improved', 'access', 'to', '#', 'technology', '&', 'amp', ';', '#', 'livelihood', '.', '#', 'ZeroHunger', '\\n', '@FAOWestAfrica', 'https://t.co/ifT8KSRo3O']\n",
      "['is', 'scaling', 'efforts', 'reducing', 'amount', 'wood', 'used', 'fuel', 'fish', 'smoking', 'stove', 'women', 'fishsmoking', 'amp', 'drying', 'industry', 'have', 'improved', 'access', 'technology', 'amp', 'livelihood']\n"
     ]
    }
   ],
   "source": [
    "tweet_id=random.choice(list(smoking.getIds()))\n",
    "smoking.getText(tweet_id)\n",
    "toks = smoking.getTokens(tweet_id)\n",
    "print([token.text for token in toks])\n",
    "filtered = smoking.getFilteredTokens(tweet_id)\n",
    "print([f.text for f in filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END OF ANSWER cut above here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Revised tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Review of some tweets might lead you to identify text patterns that might not fit with the initial tokenizing or part-of-speech tagging. Fortunately, the spacy tools provide a means for extending the tokenizer for special cases. Here, we review an example of how these tools might be used.\n",
    "\n",
    "Specifically, review of some tweets led to the following concerns: \n",
    "1. The word \"E-cigarette is split by the tokenizer into two separate tokens\n",
    "2. Hashtags are split into the pound symbol (`#`) and the following text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Tokenizing \"E-cigarette\"\n",
    "\n",
    "Consider the following tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoketweet='E-cigarette use by teens linked to later tobacco smoking, study says https://t.co/AhTpFUw0TW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '-', 'cigarette', 'use', 'by', 'teens', 'linked', 'to', 'later', 'tobacco', 'smoking', ',', 'study', 'says', 'https://t.co/AhTpFUw0TW']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(smoketweet)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"E-cigarette\" becomes three tokens. This is not what we want - we want it to be held together as one. \n",
    "To do this, we can refer to the Spacy docuentation, which describes the process for adding a [special-case tokenizer rule](https://spacy.io/usage/linguistic-features#section-tokenization). Essentially, these rules allow for the possibility of adding new rules to customize parsing for specific domains:\n",
    "\n",
    "Each new rule will be a dictionary with three fields:\n",
    "    * `ORTH` is the text that will be matched\n",
    "    * `LEMMA` is the lemma form\n",
    "    * `POS` is the part-of-speech\n",
    "    \n",
    "These can then be added to the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "nlp.tokenizer.add_special_case(u'E-cigarette', special_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text says that the text \"e-cigarette\" should be handled by the special case rule saying that it is a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e-cigarette', 'use', 'by', 'teens', 'linked', 'to', 'later', 'tobacco', 'smoking', ',', 'study', 'says', 'https://t.co/AhTpFUw0TW']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(smoketweet)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we capture \"E-cigarette\" as one token. Note the importance of including both capitalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Tokenizing hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags are important in tweets, as we might want to track frequency and trends of mentions. However, the default tokenizer does not capture hashtags as such. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@heal_crypto', ':', '#', 'VR', 'uses', 'in', 'therapy', '-', 'for', 'various', 'additictions', 'such', 'as', 'smoking', ',', 'alcohol', ',', 'overeating', ',', 'etc', '-', '#', 'HealCoin', 'https://t.co/T65Fboq7', '…']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "hashtag =\"@heal_crypto: #VR uses in therapy - for various additictions such as smoking, alcohol, overeating, etc - #HealCoin https://t.co/T65Fboq7…\"\n",
    "parsed=nlp(hashtag)\n",
    "print([tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "VR\n"
     ]
    }
   ],
   "source": [
    "print(parsed[2])\n",
    "print(parsed[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how \"#VR\" is split into \"#\" and \"VR\". To avoid this, we will can add a specialized processing component as a member of a [Spacy pipeline](https://spacy.io/usage/processing-pipelines).\n",
    "\n",
    "A pipeline is simply a set of units that operate on data in a sequential fashion. Each step in the pipeline processes the input in some way and passes it on to the next pipeline component, possibly adding some additional information. Each units can operate on the original input, or on the outputs of the earlier units. \n",
    "\n",
    "To process hashtags, we will use code suggested by a [Spacy GitHub issue](https://github.com/explosion/spaCy/issues/503). To see how this should work, let's walkt through some steps:\n",
    "\n",
    "First, let's look at the tokens in the tweet parsed above. We can iterate through with enumerate. We can also look at a few interesting elements:\n",
    "\n",
    "* `nbor` gets the next token after a token.\n",
    "* `idx ` is the position of the token in the list of characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 @heal_crypto\n",
      "12 :\n",
      "14 #\n",
      "15 VR\n"
     ]
    }
   ],
   "source": [
    "print(str(parsed[0].idx)+\" \"+parsed[0].text)\n",
    "print(str(parsed[0].nbor().idx)+\" \"+str(parsed[0].nbor().text))\n",
    "print(str(parsed[1].nbor().idx)+\" \"+str(parsed[1].nbor().text))\n",
    "print(str(parsed[2].nbor().idx)+\" \"+str(parsed[2].nbor().text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use this information to find a hash tag. essentially, we can look for a tag that has the text '#'. If we find one, \n",
    "we can look at th next tag and merge all of the characters from the start of the first tag to the end of the second tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#VR"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start=parsed[2].idx\n",
    "length = len(parsed[3].text)\n",
    "end = start+length+1\n",
    "parsed.merge(start,end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this over all of the tags, we have to repeatedly iterate over the tokens until we can't find anymore hashtags.  This leasds us ot the following routine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.nbor() is not None:\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.nbor().text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then add it to the `first` position in the pipeline, which will put it after the tokenizer, but before the part of speech tagger and other components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(hashtag_pipe,first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter\n",
      "#hashtag\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"twitter #hashtag\")\n",
    "print(doc[0].text)\n",
    "print(doc[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our original example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@heal_crypto', ':', '#VR', 'uses', 'in', 'therapy', '-', 'for', 'various', 'additictions', 'such', 'as', 'smoking', ',', 'alcohol', ',', 'overeating', ',', 'etc', '-', '#HealCoin', 'https://t.co/T65Fboq7', '…']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(hashtag)\n",
    "print([tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customization of pipelines such as this is often an important part of NLP work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
