{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media and Human-Computer Interaction - Part 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *Goal*: Use social media posts to explore the appplication of text and natural language processing to see what might be learned from online interactions.\n",
    "\n",
    "Specifically, we will retrieve, annotate, process, and interpret Twitter data on health-related issues such as depression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "References:\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n",
    "* The [Tweepy Python API for Twitter](http://www.tweepy.org/)\n",
    "\n",
    "Required Software\n",
    "* [Python 3](https://www.python.org)\n",
    "* [NumPy](http://www.numpy.org) - for preparing data for plotting\n",
    "* [Matplotlib](https://matplotlib.org) - plots and garphs\n",
    "* [jsonpickle](https://jsonpickle.github.io) for storing tweets. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This module continues the Social Media Data Science module started in [Part 1](SocialMedia - Part 1.ipynb), covering the natural language processing analysis of our tweet corpus, including \n",
    "\n",
    "  1. Natural Language Processings\n",
    "  2. Construction of classifiers\n",
    "  \n",
    "Our case study will apply these topics to Twitter discussions of smoking and vaping. Although details of the tools used to access data and the format and content of the data may differ for various services, the strategies and procedures used to analyze the data will generalize to other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Before we dig in, we must grab a bit of code from [Part 1](SocialMedia - Part 1.ipynb):\n",
    "\n",
    "1. Our Tweets class\n",
    "3. Our twitter API Keys - be sure to copy the keys that you generated when you completed [Part 1](SocialMedia - Part 1.ipynb).\n",
    "4. Configuration of our Twitter connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*REDACT FOLLOWING DETAILS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key='D2L4YZ2YrO1PMix7uKUK63b8H'\n",
    "consumer_secret='losRw9T8zb6VT3TEJ9JHmmhAmn1GXKVj30dkiMv9vjhXuiWek9'\n",
    "access_token='15283934-iggs1hiZAPI2o5sfHWMfjumTF7SvytHPjpPRGf3I6'\n",
    "access_secret='bOvqssxS97PGPwXHQZxk83KtAcDyLhRLgdQaokCdVvwFi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Examination of text patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our ultimate goal is to build a classifier capable of distinguishing tweets related to tobacco smoing from other, unrelated tweets. To do this, we will eventually build natural language processing models. However, we will start by doing some basic processing to explore the types of words and language found in the tweets. \n",
    "\n",
    "To do this, we will use the [Spacy](https://spacy.io/) Python NLP package. Spacy provides significant NLP power out-of-the box, with customization facilities offering greater flexibility at various stages of the Pipeline. Details can be found at the  [Spacy web site](https://spacy.io/), and in this [tutorial](https://nicschrading.com/project/Intro-to-NLP-with-spaCy/).\n",
    "\n",
    "However, before we get into the deails, a bit of a roadmap. \n",
    "\n",
    "Natural Language Processing involves a series of operations on an input text, each building off of the previous step to add additional insight and undertanding.  Thus, many NLP packages run as pipeline processors providing modular components at each stage of the process. Separating key steps into discrete packages provides needed modularity, as developers can modify and customize individual components as needed. Spacy, like other NLP tools including [GATE](https://gate.ac.uk/) and [cTAKES](https://ctaes.apache.org)  operate on such a model. Although the specific components of each pipeline vary from system to system (and from tasks to task, the key tasks are rougly similar:\n",
    "\n",
    "1. *Tokenizing*: splitting the text into words, punctuation, and other markers.\n",
    "2. *Part of speech tagging*: Classifying terms as nouns, verbs, adjective, adverbs, ec.\n",
    "3. *Dependency Parsing* or *Chunking*: Defining relationships between tokens (subject and object of sentence) and grouping into noun and veb phrases.\n",
    "4. *Named Entity Recognition*: Mapping words or phrases to standard vocabularies or other common, known values. This step is often key for linking free text to accepted terms for diseases, symptoms, and/or anatomic locations.\n",
    "\n",
    "Each of these steps might be accomplished through rules, machine learning models, or some combination of approaches. After these initial steps are complete, results might be used to identify relationships between items in the text, build classifiers, or otherwise conduct further analysis. We'll get into these topics later.\n",
    "\n",
    "The [Spacy documentation](https://spacy.io/usage/spacy-101) and [cTAKES default pipeline description](https://cwiki.apache.org/confluence/display/CTAKES/Default+Clinical+Pipeline) provide two examples of how these components might be arranged in practice.  For more information on NLP theory and methods, see [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/), perhaps the leading NLP textbook.\n",
    "\n",
    "Given this introduction, we can read in our tweets and get to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 Reading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of [Part 2](SocialMedia - Part 2.ipynb) you had saved two sets of tweets one for smoking and one for vaping. Let's read  in the vaping twets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vaping=Tweets()\n",
    "vaping.readTweets(\"tweets-vaping.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the smoking tweets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets-smoking.json\")\n",
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we will grab a tweet and process it. This will give us a beginning feel for what [Spacy](https://spacy.io) can do and how we might use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Zenith #Pilot Type 20 Extra Special #Cohiba-#Maduro 5 Edition watch - A smoking start to 2018 for Zenith/Cohiba partnership with Pilot Type 20 special editions \\n@ZenithWatches #Type20\\nhttps://t.co/n1CnmMTI2Y https://t.co/H82cdAb7MY'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smokingIds=list(smoking.getIds())\n",
    "tweet_id=smokingIds[11]\n",
    "sample = smoking.getText(tweet_id)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets have usage patterns that are non-standard English - URLs, hashtags, user references (this particularly tweet was not selected accidentally). These patterns create challenges for extracting content - we might want to know that \"#Type20\" is, in a tweet, a hashtag that should be considered as a complete unit.  \n",
    "\n",
    "We'll see soon how we might do this, but first, to start the NLP process, we can import the Spacy components and create an NLP object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then parse out the text from the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = nlp(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list of tokens. We can print out each token to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Zenith', '#', 'Pilot', 'Type', '20', 'Extra', 'Special', '#', 'Cohiba-#Maduro', '5', 'Edition', 'watch', '-', 'A', 'smoking', 'start', 'to', '2018', 'for', 'Zenith', '/', 'Cohiba', 'partnership', 'with', 'Pilot', 'Type', '20', 'special', 'editions', '\\n', '@ZenithWatches', '#', 'Type20', '\\n', 'https://t.co/n1CnmMTI2Y', 'https://t.co/H82cdAb7MY']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see right away that this parsing isn't quite what we would like. Default English parsing treats the `#Zenith` as two separate tokens - `#` and `Zenith`. To treat this as a hashtag, we will indeed need to revise the parser.\n",
    "\n",
    "\n",
    "Before we do that, we can also look at some of the othe attributes that we might learn form the parser. \n",
    "\n",
    "Chief among this is the *lemma_*: the \"standard\" or \"base\" form, reducing verb forms to their base verb, plurals to appropriate singular nouns, etc.  For example, the 29th token is `editions`, which has `edition` as the lemmatized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "editions\n",
      "edition\n"
     ]
    }
   ],
   "source": [
    "print(parsed[29].text)\n",
    "print(parsed[29].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pos_* and *tag_* provide basic and detailed information on the part of speech. Looking at the 7th token - \"John\", we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoking\n",
      "NOUN\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "print(parsed[15].text)\n",
    "print(parsed[15].pos_)\n",
    "print(parsed[15].tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about a part of spech tag, you can use `spacy.explain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun\n",
      "noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(parsed[15].pos_))\n",
    "print(spacy.explain(parsed[15].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pos_ is a simple  indicator of the part of speech - noun,verb, etc. \n",
    "* tag_ is a more detailed indicator of the part of speech. \n",
    "* is_stop is True if the token is a \"stop\" word - a commonly found word that might addd litle or no information.\n",
    "* is_alpha is True if the token is alphanumeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at token1 (\"@BIUK\"), token 2 (\":\"), token 6 (\"John\"), token 9 (\"how\"), and token 18 (\"scared\") to see a few more tokens in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@hunchoSr @hunchosr PROPN NNP False False\n",
      ": : PUNCT : False False\n",
      "never never PUNCT RB True True\n",
      "gun gun PUNCT NN False True\n",
      "smoking smoke VERB VBG False True\n"
     ]
    }
   ],
   "source": [
    "t1 = parsed[1]\n",
    "t2 = parsed[2]\n",
    "t5 = parsed[6]\n",
    "t9 = parsed[9]\n",
    "t17 = parsed[17]\n",
    "print (t1.text,t1.lemma_,t1.pos_,t1.tag_,t1.is_stop,t1.is_alpha)\n",
    "print (t2.text,t2.lemma_,t2.pos_,t2.tag_,t2.is_stop,t2.is_alpha)\n",
    "print (t5.text,t5.lemma_,t2.pos_,t5.tag_,t5.is_stop,t5.is_alpha)\n",
    "print (t9.text,t9.lemma_,t2.pos_,t9.tag_,t9.is_stop,t9.is_alpha)\n",
    "print (t17.text,t17.lemma_,t17.pos_,t17.tag_,t17.is_stop,t17.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations:\n",
    "* \"@BIUK\" is no alphabetical\n",
    "* \":\" is neither alphabetical or a stop word. \n",
    "* \"how\" is an alphabetical stop word. \n",
    "* \"John\" is alphabetical, but not a stop word.  \n",
    "\n",
    "Note the part-of-speech information. \"John\" has pos \"PUNCT\" and tag \"NNP\", while \"how\" has \"PUNCT\" and \"WRB\" respectively. We might look at these explanations for additional clarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punctuation\n",
      "noun, proper singular\n",
      "wh-adverb\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"PUNCT\"))\n",
    "print(spacy.explain(\"NNP\"))\n",
    "print(spacy.explain(\"WRB\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not clear why \"John\" or \"how\" would be considered punctuation, but we can guess that this is a shortocming of the machine-learning model used to determine part of speech.\n",
    "\n",
    "Fornuately, this is not necessarily a problem, as the tags provide enough information to classify these tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some NLP systems will go a bit further than Spacy's lemmatization, using a process called \"stemming\" to reduce words to base forms. With a stemming algorithm, \"scared\" might be reduced to \"scare\" - see this description of [Porter's stemming algorithm](https://tartarus.org/martin/PorterStemmer/) for more detail. \n",
    "\n",
    "Let's turn the code that we used above into a routine, along with a routine to print out token details and try another tweet or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTweetText(tweets):\n",
    "    tweet_id=random.choice(list(tweets.keys()))\n",
    "    tweet=tweets[tweet_id]['tweet']\n",
    "    return tweet['text']\n",
    "\n",
    "def printTokDetails(parsed):\n",
    "    print(\"{:30} {:30} {:7}{:7}{:7}{:7}\".format(\"Token text\",\"Lemma\",\"POS\",\"Tag\",\"Stop?\",\"Alpha?\"))\n",
    "    for tok in parsed:\n",
    "        print(\"{:30} {:30} {:7}{:7}{:7}{:7}\".format(str(tok.text),str(tok.lemma_),str(tok.pos_),str(tok.tag_),str(tok.is_stop),str(tok.is_alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample2=getTweetText(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @OnlyWayIsShawtz: My boy stopped smoking weed the day he spent 30 minutes looking for his phone under the bed.. While using his phone fl…'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed2=nlp(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "RT                             rt                             PROPN  NNP    False  True   \n",
      "@OnlyWayIsShawtz               @onlywayisshawtz               NOUN   NN     False  False  \n",
      ":                              :                              PUNCT  :      False  False  \n",
      "My                             -PRON-                         ADJ    PRP$   False  True   \n",
      "boy                            boy                            NOUN   NN     False  True   \n",
      "stopped                        stop                           VERB   VBD    False  True   \n",
      "smoking                        smoke                          VERB   VBG    False  True   \n",
      "weed                           weed                           NOUN   NN     False  True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "day                            day                            NOUN   NN     False  True   \n",
      "he                             -PRON-                         PRON   PRP    True   True   \n",
      "spent                          spend                          VERB   VBD    False  True   \n",
      "30                             30                             NUM    CD     False  False  \n",
      "minutes                        minute                         NOUN   NNS    False  True   \n",
      "looking                        look                           VERB   VBG    False  True   \n",
      "for                            for                            ADP    IN     True   True   \n",
      "his                            -PRON-                         ADJ    PRP$   True   True   \n",
      "phone                          phone                          NOUN   NN     False  True   \n",
      "under                          under                          ADP    IN     True   True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "bed                            bed                            NOUN   NN     False  True   \n",
      "..                             ..                             PUNCT  .      False  False  \n",
      "While                          while                          ADP    IN     False  True   \n",
      "using                          use                            VERB   VBG    True   True   \n",
      "his                            -PRON-                         ADJ    PRP$   True   True   \n",
      "phone                          phone                          NOUN   NN     False  True   \n",
      "fl                             fl                             X      XX     False  True   \n",
      "…                              …                              PUNCT  .      False  False  \n"
     ]
    }
   ],
   "source": [
    "printTokDetails(parsed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see some interesting pattners arising here.  For example:\n",
    "\n",
    "* We see many different type of speech. Initially, we might want to focus on the nouns alone, as they provide much of the content.  \n",
    "\n",
    "* Look for words like \"is\" or \"was\" - these might all refer to a common lemma term - \"be\", corresponding to the generic form of he verb. Do you see any other incidents of lemma forms that differ from the parsed text?\n",
    "\n",
    "* URLs and icons might be present in tweets. Are they classified as alphanumeric? Should we include them as part of the \"useful\" text from a tweet? \n",
    "\n",
    "* How should we handle the \"RT\" code for retweets, user handles, and other twitter idiosyncracies? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## EXERCISE 3.1: Filtering tokens\n",
    "\n",
    "Although NLP parsing is often a good start, further filtering is often necessary to focus on data relevant for specific tasks. In this problem, we will review some additional tweets and develop a post-processing routine capable of filtering tweets as necessary for our needs. \n",
    "\n",
    "3.1.1 Using the `getTweetText`, and `printTokDetails` routines above, aong with the spacy `parser` command, examine several tweets to decide which tokens should be included or not.  List criteria for keeeping/removing tokens. Remember to use `spacy.explain()` for any unfamiliar POS or tag entries. Note that your  criteria will not be perfect, nad will likely need refinining. Examiine enough tweets to feel confident in your criteria.\n",
    "\n",
    "3.1.2 Write a routine  `includeToken` that will return True if a token matches the criteria that you identified in 3.11, and false otherwise. Assume for now that we are only interested in nouns and verbs, as they might be a good starting point to find information about vaping or smoking.\n",
    "\n",
    "3.1.3 Write a routine `filterTweetTokens` that will filter the parsed tokens from a single tweet, returning a list of the tokens to be included, based on your criteria.\n",
    "\n",
    "3.1.4 Run `filterTweetTokens` on a few tweets. Identify any inaccuracies and explain them. When possible, identify an approach for improving performance, and implement it in a revision version of `filterTweetTokens`.\n",
    "\n",
    "3.1.5 Write a routine `parseTweets` that will iterate over the tweets in the collection. For each tweet, it will call `filterTweetTokens`, storing the resulting tweets in a list indexed by 'tokens' in the main tweet object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS Cut below here*\n",
    "\n",
    "### 3.1.1 Sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Pitsogp: Emtee must be smoking nyaope now how can u show the world yo uncircumcised dick @danielmarven  @tumisole  now the song \"Roll u…\n",
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "RT                             rt                             PROPN  NNP    False  True   \n",
      "@Pitsogp                       @pitsogp                       NOUN   NN     False  False  \n",
      ":                              :                              PUNCT  :      False  False  \n",
      "Emtee                          emtee                          NOUN   NN     False  True   \n",
      "must                           must                           VERB   MD     True   True   \n",
      "be                             be                             VERB   VB     True   True   \n",
      "smoking                        smoke                          VERB   VBG    False  True   \n",
      "nyaope                         nyaope                         NOUN   NN     False  True   \n",
      "now                            now                            ADV    RB     True   True   \n",
      "how                            how                            ADV    WRB    True   True   \n",
      "can                            can                            VERB   MD     True   True   \n",
      "u                              u                              PART   TO     False  True   \n",
      "show                           show                           VERB   VB     True   True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "world                          world                          NOUN   NN     False  True   \n",
      "yo                             yo                             ADP    IN     False  True   \n",
      "uncircumcised                  uncircumcised                  ADJ    JJ     False  True   \n",
      "dick                           dick                           NOUN   NN     False  True   \n",
      "@danielmarven                  @danielmarven                  PROPN  NNP    False  False  \n",
      "                                                              SPACE         False  False  \n",
      "@tumisole                      @tumisole                      PROPN  NNP    False  False  \n",
      "                                                              SPACE         False  False  \n",
      "now                            now                            ADV    RB     True   True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "song                           song                           NOUN   NN     False  True   \n",
      "\"                              \"                              PUNCT  ``     False  False  \n",
      "Roll                           roll                           PROPN  NNP    False  True   \n",
      "u                              u                              NOUN   NN     False  True   \n",
      "…                              …                              PUNCT  .      False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@horsekween This is like when we gave that lighter to the guy smoking spice 😂\n",
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "@horsekween                    @horsekween                    VERB   VBN    False  False  \n",
      "This                           this                           DET    DT     False  True   \n",
      "is                             be                             VERB   VBZ    True   True   \n",
      "like                           like                           ADP    IN     False  True   \n",
      "when                           when                           ADV    WRB    True   True   \n",
      "we                             -PRON-                         PRON   PRP    True   True   \n",
      "gave                           give                           VERB   VBD    False  True   \n",
      "that                           that                           DET    DT     True   True   \n",
      "lighter                        light                          ADJ    JJR    False  True   \n",
      "to                             to                             ADP    IN     True   True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "guy                            guy                            NOUN   NN     False  True   \n",
      "smoking                        smoking                        NOUN   NN     False  True   \n",
      "spice                          spice                          NOUN   NN     False  True   \n",
      "😂                              😂                              NOUN   NNS    False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ForeverGAW: 🔝BIG OP 150 FOLLOWER GAW🔝\n",
      "\n",
      "➡1100 STEAM KEYS⬅\n",
      "➡1 HELLCLIENT⬅\n",
      "➡1 FA⬅\n",
      "➡1 FA GEN⬅\n",
      "➡THE FOREST⬅\n",
      "➡1 ORGIN⬅\n",
      "➡200 MC ALTS⬅\n",
      "➡150 SPO…\n",
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "RT                             rt                             PROPN  NNP    False  True   \n",
      "@ForeverGAW                    @forevergaw                    PROPN  NNP    False  False  \n",
      ":                              :                              PUNCT  :      False  False  \n",
      "🔝                              🔝                              VERB   VB     False  False  \n",
      "BIG                            big                            NOUN   NN     False  True   \n",
      "OP                             op                             NOUN   NN     False  True   \n",
      "150                            150                            NUM    CD     False  False  \n",
      "FOLLOWER                       follower                       PROPN  NNP    False  True   \n",
      "GAW                            gaw                            PROPN  NNP    False  True   \n",
      "🔝                              🔝                              NOUN   NN     False  False  \n",
      "\n",
      "\n",
      "                             \n",
      "\n",
      "                             SPACE  _SP    False  False  \n",
      "➡                              ➡                              NOUN   NN     False  False  \n",
      "1100                           1100                           NUM    CD     False  False  \n",
      "STEAM                          steam                          PROPN  NNPS   False  True   \n",
      "KEYS                           keys                           PROPN  NNP    False  True   \n",
      "⬅                              ⬅                              PROPN  NNP    False  False  \n",
      "\n",
      "                              \n",
      "                              SPACE         False  False  \n",
      "➡                              ➡                              VERB   VBP    False  False  \n",
      "1                              1                              NUM    CD     False  False  \n",
      "HELLCLIENT                     hellclient                     PROPN  NNP    False  True   \n",
      "⬅                              ⬅                              PROPN  NNP    False  False  \n",
      "\n",
      "                              \n",
      "                              SPACE         False  False  \n",
      "➡                              ➡                              NOUN   NN     False  False  \n",
      "1                              1                              NUM    CD     False  False  \n",
      "FA                             fa                             PROPN  NNP    False  True   \n",
      "⬅                              ⬅                              PROPN  NNP    False  False  \n",
      "\n",
      "                              \n",
      "                              SPACE         False  False  \n",
      "➡                              ➡                              NOUN   NN     False  False  \n",
      "1                              1                              NUM    CD     False  False  \n",
      "FA                             fa                             PROPN  NNP    False  True   \n",
      "GEN                            gen                            PROPN  NNP    False  True   \n",
      "⬅                              ⬅                              PROPN  NNP    False  False  \n",
      "\n",
      "                              \n",
      "                              SPACE         False  False  \n",
      "➡                              ➡                              VERB   VBP    False  False  \n",
      "THE                            the                            DET    DT     False  True   \n",
      "FOREST                         forest                         PROPN  NNP    False  True   \n",
      "⬅                              ⬅                              PROPN  NNP    False  False  \n",
      "\n",
      "                              \n",
      "                              SPACE         False  False  \n",
      "➡                              ➡                              NOUN   NN     False  False  \n",
      "1                              1                              NUM    CD     False  False  \n",
      "ORGIN                          orgin                          PROPN  NNP    False  True   \n",
      "⬅                              ⬅                              PROPN  NNP    False  False  \n",
      "\n",
      "                              \n",
      "                              SPACE         False  False  \n",
      "➡                              ➡                              VERB   VBP    False  False  \n",
      "200                            200                            NUM    CD     False  False  \n",
      "MC                             mc                             PROPN  NNP    False  True   \n",
      "ALTS                           alts                           PROPN  NNP    False  True   \n",
      "⬅                              ⬅                              PROPN  NNP    False  False  \n",
      "\n",
      "                              \n",
      "                              SPACE         False  False  \n",
      "➡                              ➡                              VERB   VBP    False  False  \n",
      "150                            150                            NUM    CD     False  False  \n",
      "SPO                            spo                            NOUN   NN     False  True   \n",
      "…                              …                              PUNCT  NFP    False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @johncardillo: .@seanmdav hit a grand slam here. Great work. Smoking gun.  https://t.co/vDwjoi64Gk\n",
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "RT                             rt                             PROPN  NNP    False  True   \n",
      "@johncardillo                  @johncardillo                  PROPN  NNP    False  False  \n",
      ":                              :                              PUNCT  :      False  False  \n",
      ".@seanmdav                     .@seanmdav                     PUNCT  .      False  False  \n",
      "hit                            hit                            VERB   VBD    False  True   \n",
      "a                              a                              DET    DT     True   True   \n",
      "grand                          grand                          ADJ    JJ     False  True   \n",
      "slam                           slam                           NOUN   NN     False  True   \n",
      "here                           here                           ADV    RB     True   True   \n",
      ".                              .                              PUNCT  .      False  False  \n",
      "Great                          great                          ADJ    JJ     False  True   \n",
      "work                           work                           NOUN   NN     False  True   \n",
      ".                              .                              PUNCT  .      False  False  \n",
      "Smoking                        smoking                        NOUN   NN     False  True   \n",
      "gun                            gun                            NOUN   NN     False  True   \n",
      ".                              .                              PUNCT  .      False  False  \n",
      "                                                              SPACE         False  False  \n",
      "https://t.co/vDwjoi64Gk        https://t.co/vdwjoi64gk        PROPN  NNP    False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria: \n",
    "    \n",
    "* Alpha is true, and \n",
    "* Stop is false, and \n",
    "* text is not \"RT\"\n",
    "* Tag is NN, Tag is NNP, or POS is VERB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2  `includeToken`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our routine will accept a token only if it meets the criteria given above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_alpha == True and tok.is_stop == False:\n",
    "        if tok.text =='RT':\n",
    "            val = False\n",
    "        elif tok.tag_ =='NN' or tok.tag_=='NNP' or tok.pos_=='VERB':\n",
    "            val = True\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whad you do after smoking some new staff😂😂😂 https://t.co/9HIWJJMoUT\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "parsed=nlp(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[0])\n",
    "includeToken(parsed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[1])\n",
    "includeToken(parsed[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[2])\n",
    "includeToken(parsed[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whad True\n",
      "you False\n",
      "do False\n",
      "after False\n",
      "smoking True\n",
      "some False\n",
      "new False\n",
      "staff True\n",
      "😂 False\n",
      "😂 False\n",
      "😂 False\n",
      "https://t.co/9HIWJJMoUT False\n"
     ]
    }
   ],
   "source": [
    "for tok in parsed:\n",
    "    print(tok,includeToken(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looks ok. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Write a routine `filterTweeTokens` that will parse a single tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for tok in tokens:\n",
    "        if includeToken(tok) == True:\n",
    "            filtered.append(tok)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whad you do after smoking some new staff😂😂😂 https://t.co/9HIWJJMoUT\n",
      "Whad\n",
      "smoking\n",
      "staff\n"
     ]
    }
   ],
   "source": [
    "f= filterTweetTokens(parsed)\n",
    "print(sample)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Run `filterTweetTokens` on a few tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @kingamajorr: Girls have bible verses in their bios on their main instas, but videos of them drinking and smoking on their finstas... th…\n",
      "drinking\n",
      "smoking\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @chocoo_loco: I just want my friends to stop smoking weed😂 https://t.co/LWI2HVofAf\n",
      "want\n",
      "stop\n",
      "smoking\n",
      "weed\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @HealthyAirUK: Is being exposed to air pollution like passive smoking? #HelpBritainBreathe https://t.co/WTYfADhJ68\n",
      "Is\n",
      "exposed\n",
      "air\n",
      "pollution\n",
      "smoking\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, inclusion of \"Do\" looks questionable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fits the criteria. we might ask ourselves if we want verbs, but we do want to include 'smoke','vape', etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @FootyMemes: This new anti-smoking ad is really powerful... https://t.co/pWHZDLIb7O\n",
      "smoking\n",
      "ad\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @artofdai: My version of Van Gogh's \"head of a skeleton smoking a cigarette\" https://t.co/jQvaQQSg3P\n",
      "version\n",
      "Van\n",
      "Gogh\n",
      "head\n",
      "skeleton\n",
      "smoking\n",
      "cigarette\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @foxnewspolitics: 'Smoking gun'email shows Obama DOJ blocked conservative groups from settlement funds,GOP lawmaker says- @AlexPappas\n",
      "ht…\n",
      "Smoking\n",
      "shows\n",
      "Obama\n",
      "DOJ\n",
      "blocked\n",
      "settlement\n",
      "GOP\n",
      "lawmaker\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(tweets)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5  `parseTweets` routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseTweets(tweets):\n",
    "    for tweet_id in tweets:\n",
    "        tweet= tweets[tweet_id]\n",
    "        text = tweet['tweet']['text']\n",
    "        parsed=nlp(text)\n",
    "        filtered = filterTweetTokens(parsed)\n",
    "        tweet['filtered'] = filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parseTweets(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_id=random.choice(list(tweets.keys()))\n",
    "tweet=tweets[tweet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6✨#ℹnfographics for #pot lovers\\nhttps://t.co/dgPS5FFIwR\\n#weed #marijuana #cannabis #smoking #vaping #herbs #herbal… https://t.co/sjh7HqDMVe'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['tweet']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pot, weed, marijuana, cannabis, smoking, vaping, herbs]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet['filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @onmyworst: Not to sound like tana mongeau but I love smoking weed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[sound, tana, mongeau, love, smoking, weed]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id=random.choice(list(tweets.keys()))\n",
    "tweet=tweets[tweet_id]\n",
    "print(tweet['tweet']['text'])\n",
    "tweet['filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@komodoteen4u2nv @sleepbutt ooh get a pax! they look like a juul vape so v easy to smoke anywhere\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[pax, look, juul, vape, smoke]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id=random.choice(list(tweets.keys()))\n",
    "tweet=tweets[tweet_id]\n",
    "print(tweet['tweet']['text'])\n",
    "tweet['filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @VersaceVersacei: I'm in the shower and my girl is doing her makeup in here and we're smoking a wood together 💕 I love her so much 🖤 htt…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[shower, girl, makeup, smoking, wood, love, htt]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id=random.choice(list(tweets.keys()))\n",
    "tweet=tweets[tweet_id]\n",
    "print(tweet['tweet']['text'])\n",
    "tweet['filtered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@JedSanborn I suspect being able to regularly see a doctor might also lower those rates of smoking and obesity. #publichealth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[suspect, doctor, lower, smoking, obesity, publichealth]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id=random.choice(list(tweets.keys()))\n",
    "tweet=tweets[tweet_id]\n",
    "print(tweet['tweet']['text'])\n",
    "tweet['filtered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweet_id=random.choice(list(tweets.keys()))\n",
    "tweet=tweets[tweet_id]\n",
    "print(tweet['tweet']['text'])\n",
    "tweet['filtered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END OF ANSWER cut above here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Revised tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Review of some tweets might lead you to identify text patterns that might not fit with the initial tokenizing or part-of-speech tagging. Fortunately, the spacy tools provide a means for extending the tokenizer for special cases. Here, we review an example of how these tools might be used.\n",
    "\n",
    "Specifically, review of some tweets led to the following concerns: \n",
    "1. The word \"E-cigarette is split by the tokenizer into two separate tokens\n",
    "2. Hashtags are split into the pound symbol (`#`) and the following text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Tokenizing \"E-cigarette\"\n",
    "\n",
    "Consider the following tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smoketweet='E-cigarette use by teens linked to later tobacco smoking, study says https://t.co/AhTpFUw0TW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '-', 'cigarette', 'use', 'by', 'teens', 'linked', 'to', 'later', 'tobacco', 'smoking', ',', 'study', 'says', 'https://t.co/AhTpFUw0TW']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(smoketweet)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"E-cigarette\" becomes three tokens. This is not what we want - we want it to be held together as one. \n",
    "To do this, we can add a [special-case tokenizer rule](https://spacy.io/usage/linguistic-features#section-tokenization) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "nlp.tokenizer.add_special_case(u'E-cigarette', special_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text says that the text \"e-cigarette\" should be handled by the special case rule saying that it is a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e-cigarette', 'use', 'by', 'teens', 'linked', 'to', 'later', 'tobacco', 'smoking', ',', 'study', 'says', 'https://t.co/AhTpFUw0TW']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(smoketweet)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we capture \"E-cigarette\" as one token. Note the importance of including both capitalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Tokenizing hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags are important in tweets, as we might want to track frequency and trends of mentions. However, the default tokenizer does not capture hashtags as such. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@heal_crypto', ':', '#', 'VR', 'uses', 'in', 'therapy', '-', 'for', 'various', 'additictions', 'such', 'as', 'smoking', ',', 'alcohol', ',', 'overeating', ',', 'etc', '-', '#', 'HealCoin', 'https://t.co/T65Fboq7', '…']\n"
     ]
    }
   ],
   "source": [
    "hashtag =\"RT @heal_crypto: #VR uses in therapy - for various additictions such as smoking, alcohol, overeating, etc - #HealCoin https://t.co/T65Fboq7…\"\n",
    "parsed=nlp(hashtag)\n",
    "print([tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how \"#VR\" is split into \"#\" and \"VR\". To avoid this, we will can add a specialized [spacy pipeline](https://github.com/explosion/spaCy/issues/503)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.nbor() is not None:\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.nbor().text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "nlp.add_pipe(hashtag_pipe)\n",
    "\n",
    "doc = nlp(\"twitter #hashtag\")\n",
    "assert len(doc) == 2\n",
    "assert doc[0].text == 'twitter'\n",
    "assert doc[1].text == '#hashtag'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine looks at tokens starting with '#' and adds the \"nbor\" - the next token - to it. This is added to the [spacy pipeline](https://spacy.io/usage/processing-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our original example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@heal_crypto', ':', '#VR', 'uses', 'in', 'therapy', '-', 'for', 'various', 'additictions', 'such', 'as', 'smoking', ',', 'alcohol', ',', 'overeating', ',', 'etc', '-', '#HealCoin', 'https://t.co/T65Fboq7', '…']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(hashtag)\n",
    "print([tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customization of pipelines such as this is often an important part of NLP work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
