{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH, LEMMA, POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "    \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    vape_case = [{ORTH: u'vape',LEMMA:u'vape',POS: u'NOUN'}]\n",
    "    \n",
    "    vape_spellings =[u'vap',u'vape',u'vaping',u'vapor',u'Vap',u'Vape',u'Vapor',u'Vapour']\n",
    "    for v in vape_spellings:\n",
    "        nlp.tokenizer.add_special_case(v, vape_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets-smoking.json\")\n",
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping=Tweets()\n",
    "vaping.readTweets(\"tweets-vaping.json\")\n",
    "vaping.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. so this is a bit opaque.  is sci-kit any clearer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the scikit spacy combination here ---\n",
    "\n",
    "https://nicschrading.com/project/Intro-to-NLP-with-spaCy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \n",
    "        \"lol @twitterdude that is gr8\", \"twitter &amp; reddit are fun.\", \n",
    "        \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \n",
    "        \"Rockets launch from Earth and go to other planets.\", \"twitter social media &gt; &lt;\", \n",
    "        \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
    "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
    "\n",
    "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
    "labelsTest = [\"twitter\", \"space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 23, 'space': 39, 'is': 18, 'great': 13, 'planets': 31, 'are': 3, 'cool': 5, 'am': 0, 'glad': 10, 'they': 43, 'exist': 7, 'in': 17, 'lol': 22, 'twitterdude': 46, 'that': 41, 'gr8': 12, 'twitter': 45, 'amp': 1, 'reddit': 33, 'fun': 9, 'mars': 25, 'planet': 30, 'it': 19, 'red': 32, 'microsoft': 27, 'skip': 35, 'windows': 47, 'rockets': 34, 'launch': 20, 'from': 8, 'earth': 6, 'and': 2, 'go': 11, 'to': 44, 'other': 29, 'social': 36, 'media': 26, 'gt': 15, 'lt': 24, 'someguy': 38, 'somegirl': 37, 'hashtag': 16, 'orbiting': 28, 'the': 42, 'sun': 40, 'little': 21, 'blue': 4, 'green': 14}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am',\n",
       " 'amp',\n",
       " 'and',\n",
       " 'are',\n",
       " 'blue',\n",
       " 'cool',\n",
       " 'earth',\n",
       " 'exist',\n",
       " 'from',\n",
       " 'fun',\n",
       " 'glad',\n",
       " 'go',\n",
       " 'gr8',\n",
       " 'great',\n",
       " 'green',\n",
       " 'gt',\n",
       " 'hashtag',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'launch',\n",
       " 'little',\n",
       " 'lol',\n",
       " 'love',\n",
       " 'lt',\n",
       " 'mars',\n",
       " 'media',\n",
       " 'microsoft',\n",
       " 'orbiting',\n",
       " 'other',\n",
       " 'planet',\n",
       " 'planets',\n",
       " 'red',\n",
       " 'reddit',\n",
       " 'rockets',\n",
       " 'skip',\n",
       " 'social',\n",
       " 'somegirl',\n",
       " 'someguy',\n",
       " 'space',\n",
       " 'sun',\n",
       " 'that',\n",
       " 'the',\n",
       " 'they',\n",
       " 'to',\n",
       " 'twitter',\n",
       " 'twitterdude',\n",
       " 'windows']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get(\"windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=vectorizer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 48)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 1 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 23, 'space': 39, 'is': 18, 'great': 13, 'planets': 31, 'are': 3, 'cool': 5, 'am': 0, 'glad': 10, 'they': 43, 'exist': 7, 'in': 17, 'lol': 22, 'twitterdude': 46, 'that': 41, 'gr8': 12, 'twitter': 45, 'amp': 1, 'reddit': 33, 'fun': 9, 'mars': 25, 'planet': 30, 'it': 19, 'red': 32, 'microsoft': 27, 'skip': 35, 'windows': 47, 'rockets': 34, 'launch': 20, 'from': 8, 'earth': 6, 'and': 2, 'go': 11, 'to': 44, 'other': 29, 'social': 36, 'media': 26, 'gt': 15, 'lt': 24, 'someguy': 38, 'somegirl': 37, 'hashtag': 16, 'orbiting': 28, 'the': 42, 'sun': 40, 'little': 21, 'blue': 4, 'green': 14}\n",
      "[2.70474809 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 1.78845736 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.01160091 2.70474809 2.70474809]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(train)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 48)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43323568 0.         0.         0.         0.\n",
      "  0.28646791 0.         0.         0.         0.         0.43323568\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.73657982 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.34989927 0.         0.         0.29744635 0.         0.34989927\n",
      "  0.         0.34989927 0.         0.         0.34989927 0.\n",
      "  0.         0.         0.         0.         0.         0.34989927\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29744635 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29744635 0.         0.\n",
      "  0.         0.34989927 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.47472745 0.         0.         0.         0.         0.\n",
      "  0.31390347 0.         0.         0.         0.47472745 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.47472745\n",
      "  0.         0.         0.         0.         0.47472745 0.        ]\n",
      " [0.         0.48360622 0.         0.41110947 0.         0.\n",
      "  0.         0.         0.         0.48360622 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48360622 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.3596722  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.56536198 0.42750858 0.         0.         0.         0.\n",
      "  0.         0.42750858 0.         0.         0.         0.\n",
      "  0.36342135 0.         0.42750858 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.57735027\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.57735027]\n",
      " [0.         0.         0.33859118 0.         0.         0.\n",
      "  0.33859118 0.         0.33859118 0.         0.         0.33859118\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33859118 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33859118\n",
      "  0.         0.28783344 0.         0.         0.33859118 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33859118 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.46864588 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.46864588 0.         0.46864588 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.46864588 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34854576 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.53051081 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.53051081 0.53051081 0.         0.         0.\n",
      "  0.         0.         0.         0.39455653 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.37372071 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37372071 0.         0.         0.\n",
      "  0.2471149  0.         0.         0.37372071 0.         0.\n",
      "  0.         0.         0.         0.         0.37372071 0.\n",
      "  0.31769674 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37372071 0.\n",
      "  0.37372071 0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform(train)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. now -- how can I train on this? \n",
    "and how can I pass in my spacy tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plan\n",
    "\n",
    "1. Go back to https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "\n",
    "1.1 create NLP as per part 3 with hashtag tokenizer.\n",
    "\n",
    "1.2. wrap that in a tokenizer that calls the spacy NLP parser and tokenizes with stops, etc.\n",
    "\n",
    "1.3 use that tokenizer in a tf-df\n",
    "\n",
    "1.4 follow that up with classifier. \n",
    "\n",
    "1.5 test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = True\n",
    "    while merged_hashtag == True:\n",
    "        merged_hashtag = False\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                try:\n",
    "                    nbor = token.nbor()\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.nbor().text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "    return doc\n",
    "nlp.add_pipe(hashtag_pipe,first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_stop == False:\n",
    "        if tok.is_alpha == True: \n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='PROPN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        elif tok.text[0]=='#' or tok.text[0]=='@':\n",
    "            val = True\n",
    "    if val== True:\n",
    "        stripped =tok.lemma_.lower().strip()\n",
    "        if len(stripped) ==0:\n",
    "            val = False\n",
    "        else:\n",
    "            val = stripped\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(sample):\n",
    "    tokens = nlp(sample)\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= getTwitterNLP()\n",
    "\n",
    "def filterTweetTokens2(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    for ent in tokens.ents:\n",
    "        stripped =ent.text.strip()\n",
    "        if len(stripped) >0:\n",
    "            filtered.append(ent.text.strip())\n",
    "    return filtered\n",
    "\n",
    "def tokenizeText2(text):\n",
    "    tokens=nlp(text)\n",
    "    return filterTweetTokens2(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 - set up tokenizer in a vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= TfidfVectorizer(tokenizer=tokenizeText)\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=tokenizeText2)\n",
    "clf = LinearSVC()\n",
    "clf2 = LinearSVC()\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
    "pipe2= Pipeline([('vectorizer', vectorizer2), ('clf', clf2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 now I would do vectorizer. fit - need good input and labels.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tweets at 0x1085989b0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tweets at 0x108598978>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ok. to do this I think the following needs to happen\n",
    "1. flatten the lists into pairs of text and category\n",
    "2. shuffle and pick 0.8 of each contaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenTweets(tweets):\n",
    "    flat=[]\n",
    "    for i in tweets.getIds():\n",
    "        text = tweets.getText(i)\n",
    "        cat = tweets.getSearchTerm(i) \n",
    "        pair =(text,cat)\n",
    "        flat.append(pair)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrainSplit(pairs,splitFactor=0.8):\n",
    "    random.shuffle(pairs)\n",
    "    split=int(len(pairs)*splitFactor)\n",
    "    train=pairs[:split]\n",
    "    test =pairs[split:]\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrain(smoking,vaping):\n",
    "    fsmoke=flattenTweets(smoking)\n",
    "    fvape=flattenTweets(vaping)\n",
    "    strain,stest=getTestTrainSplit(fsmoke)\n",
    "    vtrain,vtest=getTestTrainSplit(fvape)\n",
    "    train=strain+vtrain\n",
    "    test=stest+vtest\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(test)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok. split established, but something is not right. need to ensure right split. \n",
    "# an then will need to split train and test into cateogires and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=getTestTrain(smoking,vaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTweets,trainCats=zip(*train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Smoking numbers hit new low as Britons turn to #vaping to help quit cigarettes 😀👍 https://t.co/MylD9njUZo'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainTweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainCats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vaping',\n",
       " 'smoking',\n",
       " 'smoking',\n",
       " 'vaping',\n",
       " 'smoking',\n",
       " 'smoking',\n",
       " 'smoking',\n",
       " 'vaping',\n",
       " 'smoking',\n",
       " 'vaping')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCats[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try to run through the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(trainTweets,trainCats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ok. did that do anything? Let's get the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweets,testCats=zip(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipe.predict(testTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", accuracy_score(testCats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. 97.5% accurate classification. can we look at the mismatches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip up input, category, prediction.  Look at first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I would probably describe Burning Embers as what people who are smoking weed would hear if they listened to Fortune’s Embers. Kind of a fitting name, now that I think about it... https://t.co/Sfifx4NIy6',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('I’m crying what kind stuff y’all smoking 😂😭 https://t.co/hkRB0z9i6E',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('RT MtBakerVapor Get #MtBakerVapor at https://t.co/cZo4rT0bgU \"NOW AVAILABLE - yamivapor - 8 different flavors, sure to satisfy your taste buds! #vaping #ejuice\\n.\\nGet yours now: https://t.co/QH72YlTdZE https://t.co/i6JGyGy649\" #ejuice #eliquid #vape',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Say no to smoking kids', 'smoking', 'smoking'),\n",
       " ('To celebrate #VApril we’re giving one lucky #vaper a personalised Vapor Space Medium Vapor Box to store their #vaping goodies in! To enter to #win just follow + RT. #Giveaway https://t.co/xJ7mhcs6JG',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Vapor Joes - Daily Vaping Deals: EXTENDED: OM VAPORS 75% OFF LIQUID ($4.99 PER 120M... https://t.co/paClIydHie',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('TonkaCARES is hosting a Vaping &amp; E-Cig Awareness Event tomorrow night from 6-8pm @ Mtka High School.  @TonkaCARES @TonkaSchools @MinnetonkaMN https://t.co/bN6mc1BKIt',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('I never said it was promoting it but I would rather them post vaping videos over videos of them smoking cigarettes, weed, or anything else. https://t.co/Glta7sImTq',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('\"\\'Weak\\' evidence linking e-cigarette use with future smoking\"\\n\\nUsing e-cigarettes is a recommended way of giving up smoking, but can their use make children more likely to take up the real thing? @nhschoices looks at the evidence\\n\\nhttps://t.co/Mh22zscOBI\\n\\n#behindtheheadlines https://t.co/3h5FetrYWi',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('sexy girls droping there pants pictures of girls smoking virginia slims cigarettes https://t.co/RXYgAr8rKZ',\n",
       "  'smoking',\n",
       "  'smoking')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "reslist=list(res)\n",
    "reslist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out those that don't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "for (sample,label,pred) in res:\n",
    "    if label != pred:\n",
    "        print(sample+\", \"+label+\", \"+pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. must put this into the text to challenge people to make the record better.  Then, challenge them to annotate and filter.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simlar for pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.fit(trainTweets,trainCats)\n",
    "preds = pipe2.predict(testTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", accuracy_score(testCats, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I would probably describe Burning Embers as what people who are smoking weed would hear if they listened to Fortune’s Embers. Kind of a fitting name, now that I think about it... https://t.co/Sfifx4NIy6',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('I’m crying what kind stuff y’all smoking 😂😭 https://t.co/hkRB0z9i6E',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('RT MtBakerVapor Get #MtBakerVapor at https://t.co/cZo4rT0bgU \"NOW AVAILABLE - yamivapor - 8 different flavors, sure to satisfy your taste buds! #vaping #ejuice\\n.\\nGet yours now: https://t.co/QH72YlTdZE https://t.co/i6JGyGy649\" #ejuice #eliquid #vape',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Say no to smoking kids', 'smoking', 'smoking'),\n",
       " ('To celebrate #VApril we’re giving one lucky #vaper a personalised Vapor Space Medium Vapor Box to store their #vaping goodies in! To enter to #win just follow + RT. #Giveaway https://t.co/xJ7mhcs6JG',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Vapor Joes - Daily Vaping Deals: EXTENDED: OM VAPORS 75% OFF LIQUID ($4.99 PER 120M... https://t.co/paClIydHie',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('TonkaCARES is hosting a Vaping &amp; E-Cig Awareness Event tomorrow night from 6-8pm @ Mtka High School.  @TonkaCARES @TonkaSchools @MinnetonkaMN https://t.co/bN6mc1BKIt',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('I never said it was promoting it but I would rather them post vaping videos over videos of them smoking cigarettes, weed, or anything else. https://t.co/Glta7sImTq',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('\"\\'Weak\\' evidence linking e-cigarette use with future smoking\"\\n\\nUsing e-cigarettes is a recommended way of giving up smoking, but can their use make children more likely to take up the real thing? @nhschoices looks at the evidence\\n\\nhttps://t.co/Mh22zscOBI\\n\\n#behindtheheadlines https://t.co/3h5FetrYWi',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('sexy girls droping there pants pictures of girls smoking virginia slims cigarettes https://t.co/RXYgAr8rKZ',\n",
       "  'smoking',\n",
       "  'smoking')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "reslist=list(res)\n",
    "reslist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "               'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
